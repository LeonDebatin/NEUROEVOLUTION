{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "seed=0\n",
    "import random\n",
    "random.seed(seed)\n",
    "import torch\n",
    "import pickle\n",
    "torch.manual_seed(seed)\n",
    "X_train = pd.read_csv('datamart/X_train_clipped_scaled.csv').values\n",
    "y_train = pd.read_csv('datamart/y_lactose_train.csv').values\n",
    "X_test = pd.read_csv('datamart/X_test_clipped_scaled.csv').values\n",
    "y_test = pd.read_csv('datamart/y_lactose_test.csv').values\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "      layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cross_validation(X_train, y_train, model, loss, optimizer_name, lr=None, weight_decay=0.0, n_epochs=1000, batch_size=10,  kf=None, log=False, id=None ):\n",
    "\n",
    "    if log:\n",
    "        name = 'logs/NN/' + f'{id}' + '_random_search.csv'\n",
    "        with open(name, 'w', newline='\\n') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=';')\n",
    "            w.writerow(['id']+['fold']+['best_score']+['best_generations'])\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        X_train_cross, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_train_cross, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        model.apply(reset_weights) \n",
    "        # Initialize the optimizer, we have to do this each fold otherwise the optimizer will continue from where it left off\n",
    "        #this is also why wi initialize it within the cross validaion \n",
    "        optimizer = get_optimizer(model.parameters(),optimizer_name, lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        batch_start = torch.arange(0, len(X_train_cross), batch_size)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                bar.set_description(f\"Epoch {epoch}\")\n",
    "                for start in bar:\n",
    "                    # Take a batch\n",
    "                    X_batch = X_train_cross[start:start+batch_size]\n",
    "                    y_batch = y_train_cross[start:start+batch_size]\n",
    "                    # Forward pass\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    # Backward pass\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update weights\n",
    "                    \n",
    "                    # Print progress\n",
    "                    bar.set_postfix(mse=float(loss))\n",
    "            \n",
    "            # Evaluate accuracy at end of each epoch\n",
    "            model.eval()\n",
    "            y_pred = model(X_val)\n",
    "            mse = loss_fn(y_pred, y_val)\n",
    "            mse = float(mse)    \n",
    "            if log:\n",
    "                name = 'logs/NN/' + f'{id}'+ '_random_search.csv'\n",
    "                with open(name, 'a', newline='\\n') as csvfile:\n",
    "                    w = csv.writer(csvfile, delimiter=';')\n",
    "                    w.writerow([id]+[fold]+[mse]+[epoch])\n",
    "                    \n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "                \n",
    "        results.append(mse)\n",
    "        \n",
    "    avg_mse = np.mean(results)\n",
    "    print(avg_mse)\n",
    "    return avg_mse\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(12, 1),\n",
    "        nn.Linear(1, 1),\n",
    "    )\n",
    "\n",
    "def eval_model(model, X_test, y_test):\n",
    "    pred = model.forward(X_test)[:, 0]\n",
    "    return mean_squared_error(y_test, pred)\n",
    "\n",
    "def create_random_model():\n",
    "    n_neurons_1 = random.randint(1,10)\n",
    "    n_neurons_2 = random.randint(1,10)\n",
    "    r1 = random.uniform(0,1)\n",
    "    r2 = random.uniform(0,1)\n",
    "    model = nn.Sequential(nn.Linear( 12, n_neurons_1))\n",
    "    \n",
    "    if r1 < 0.25:\n",
    "        model.add_module('relu1', nn.ReLU())\n",
    "        \n",
    "    elif 0.25 < r1< 0.5:\n",
    "        model.add_module('sigmoid1', nn.Sigmoid())\n",
    "    \n",
    "    if 0.5 < r1 < 0.75:\n",
    "        model.add_module('linear', nn.Linear(n_neurons_1, n_neurons_2))\n",
    "        \n",
    "        if r2 < 0.25:\n",
    "            model.add_module('relu2', nn.ReLU())\n",
    "        elif 0.25 < r2 < 0.5:\n",
    "            model.add_module('sigmoid2', nn.Sigmoid())\n",
    "        model.add_module('output', nn.Linear(n_neurons_2, 1))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    model.add_module('output', nn.Linear(n_neurons_1,1))\n",
    "    # if r2 > 0.25:\n",
    "    #     model.add_module('relu2', nn.ReLU())\n",
    "    # elif 0.25 < r2 < 0.5:\n",
    "    #     model.add_module('sigmoid2', nn.Sigmoid())\n",
    "    return model\n",
    "\n",
    "def get_optimizer(model_params, optimizer_name, lr, weight_decay=0.0):\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return optim.SGD(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'ASGD':\n",
    "        return optim.ASGD(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        return optim.RMSprop(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer name')\n",
    "\n",
    "def random_optimizer():\n",
    "    r = random.uniform(0,1)\n",
    "    if r < 0.2:\n",
    "        return 'Adam'\n",
    "    elif 0.2 < r < 0.4:\n",
    "        return 'SGD'\n",
    "    elif 0.4 < r < 0.6:\n",
    "        return 'Adadelta'\n",
    "    elif 0.6 < r < 0.8:\n",
    "        return 'ASGD'\n",
    "    return 'RMSprop'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(X_train, y_train, model, loss_fn, optimizer_name, lr=None, weight_decay=0.0, n_epochs=1000, batch_size=10, log=False, id=None):\n",
    "    \n",
    "    if log:\n",
    "        name = 'logs/NN/' + f'{id}' + '_training_log.csv'\n",
    "        with open(name, 'w', newline='\\n') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=';')\n",
    "            w.writerow(['id', 'epoch', 'train_loss'])\n",
    "    \n",
    "    # Reset model weights\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    \n",
    "    optimizer = get_optimizer(model.parameters(), optimizer_name, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # Take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Store the batch loss\n",
    "                train_losses.append(loss.item())\n",
    "                \n",
    "\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        if log:\n",
    "            with open(name, 'a', newline='\\n') as csvfile:\n",
    "                w = csv.writer(csvfile, delimiter=';')\n",
    "                w.writerow([id, epoch, avg_train_loss])\n",
    "        \n",
    "        # Save best model\n",
    "        \n",
    "    with open('models/NN/'+str(id)+'_NN.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036237230873666705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0036237230873666705"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.001\n",
    "n_epochs = 400   # number of epochs to run\n",
    "batch_size = 10   # size of each batch\n",
    "optimizer_name = 'Adam'\n",
    "# Early stopping parameters\n",
    "patience = 50  # how many epochs to wait after last improvement\n",
    "weight_decay = 0.01  # L2 regularization\n",
    "# K-Fold Cross Validation\n",
    "kf = RepeatedKFold(n_splits=10, random_state=seed, n_repeats=2)\n",
    "fold_results = []\n",
    "nn_cross_validation(X_train=X_train, y_train=y_train, model=model, loss=loss_fn, optimizer_name=optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, kf=kf, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02926812502555549\n",
      "Iteration 0\n",
      "New best validation score: 0.02926812502555549\n",
      "New best test score: 0.045174211263656616\n",
      "0.0036066928587388247\n",
      "Iteration 1\n",
      "New best validation score: 0.0036066928587388247\n",
      "New best test score: 0.003363759024068713\n",
      "0.0037389357457868755\n",
      "0.24434138759970664\n",
      "0.223229239275679\n",
      "0.0037171069358009844\n",
      "0.008428127027582378\n",
      "0.00923850117251277\n",
      "0.003971402707975358\n",
      "23.738884353637694\n",
      "0.007990941486787051\n",
      "3.2851772860391066\n",
      "0.008130776463076472\n",
      "0.020705861295573412\n",
      "0.006316631601657718\n",
      "21.484795570373535\n",
      "23.01572608947754\n",
      "0.00446481762919575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(n_epoches)\n\u001b[0;32m     24\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(weight_decays)\n\u001b[1;32m---> 25\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mnn_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/NN/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_search\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mnn_cross_validation\u001b[1;34m(X_train, y_train, model, loss, optimizer_name, lr, weight_decay, n_epochs, batch_size, kf, log, id)\u001b[0m\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\leond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = RepeatedKFold(n_splits=10, random_state=seed, n_repeats=2)\n",
    "batch_sizes = [5,10,15,20,25]\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "n_epoches = [25, 50, 75, 100, 250, 500, 1000]\n",
    "weight_decays = [0.0, 0.1, 0.01, 0.001, 0.0001] \n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_score = np.inf\n",
    "\n",
    "name = 'logs/NN/' + 'random_search' + '.csv'\n",
    "with open(name, 'w', newline='\\n') as csvfile:\n",
    "    w = csv.writer(csvfile, delimiter=';')\n",
    "    w.writerow(['id']+['model']+['optimizer']+['learning_rate']+['l2']+['batch_size']+['n_epochs']+['cv_score'])\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    model = create_random_model()\n",
    "    lr = random.choice(learning_rates)\n",
    "    optimizer_name = random_optimizer()\n",
    "    n_epochs = random.choice(n_epoches)\n",
    "    weight_decay = random.choice(weight_decays)\n",
    "    score = nn_cross_validation(X_train=X_train, y_train=y_train, model=model, loss=loss_fn, optimizer_name = optimizer_name,lr=lr,  n_epochs=n_epochs, batch_size=batch_size,kf=kf, log=True, id=i)\n",
    "    \n",
    "    name = 'logs/NN/' + 'random_search' + '.csv'\n",
    "    with open(name, 'a', newline='\\n') as csvfile:\n",
    "        w = csv.writer(csvfile, delimiter=';')\n",
    "        w.writerow([i]+[f'{model}']+[f'{optimizer_name}']+[lr]+[weight_decay]+[batch_size]+[n_epochs]+[score])\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        print('Iteration '+ str(i))\n",
    "        print(f'New best validation score: {best_score}')\n",
    "        \n",
    "        nn_train(X_train = X_train, y_train= y_train, model=model, loss_fn = loss_fn, optimizer_name = optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, log=True, id=i)\n",
    "        model = pickle.load(open('models/NN/' + str(i) + '_NN.pkl', 'rb'))\n",
    "        y_test_pred = model(X_test)\n",
    "        test_score = loss_fn(y_test_pred, y_test)\n",
    "        test_score = float(test_score)\n",
    "        print(f'New best test score: {test_score}')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                              model optimizer  \\\n",
      "0    0  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "1    1  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "2    2  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "3    3  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "4    4  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "5    5  Sequential(\\n  (0): Linear(in_features=12, out...      ASGD   \n",
      "6    6  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "7    7  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "8    8  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "9    9  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "10  10  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "11  11  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "12  12  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "13  13  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "14  14  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "15  15  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "16  16  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "17  17  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "18  18  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "19  19  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "20  20  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "21  21  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "22  22  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "23  23  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "24  24  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "\n",
      "    learning_rate      l2  batch_size  n_epochs   cv_score  \n",
      "0          0.1000  0.0100          20      1000   0.027185  \n",
      "1          0.0010  0.0000          20      1000   0.003589  \n",
      "2          0.0010  0.0000          25       500   0.003705  \n",
      "3          0.0010  0.0010          15       100   0.359271  \n",
      "4          0.0001  0.0010          25      1000   1.042004  \n",
      "5          0.0100  0.1000           5        25   0.003779  \n",
      "6          0.1000  0.0001          25        75   0.004837  \n",
      "7          0.0100  0.1000          20       250   0.011469  \n",
      "8          0.1000  0.0100          25        50   0.003878  \n",
      "9          0.0100  0.1000          10        25  24.292772  \n",
      "10         0.1000  0.0100          10       250   0.005068  \n",
      "11         0.1000  0.0010          25       100  10.860096  \n",
      "12         0.0100  0.0000          15        50   0.007870  \n",
      "13         0.1000  0.1000          15        25   0.047261  \n",
      "14         0.0001  0.1000          10       500   0.013749  \n",
      "15         0.0001  0.1000          25        25  21.433744  \n",
      "16         0.0001  0.0000          10       250  23.871287  \n",
      "17         0.1000  0.0010          15       250   0.004341  \n",
      "18         0.0100  0.0010           5       500   0.003556  \n",
      "19         0.0010  0.0001          25        75   4.811849  \n",
      "20         0.0001  0.0001          15       100  21.466980  \n",
      "21         0.0010  0.0010          25        25  24.654946  \n",
      "22         0.1000  0.0100           5        75   0.004474  \n",
      "23         0.1000  0.0000          25        25   0.038439  \n",
      "24         0.1000  0.0001          25       500   0.006457  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('logs/NN/random_search.csv', delimiter=';')\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[df['cv_score']<0.005, 'cv_score'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                              model optimizer  \\\n",
      "18  18  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "\n",
      "    learning_rate     l2  batch_size  n_epochs  cv_score  \n",
      "18           0.01  0.001           5       500  0.003556  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.loc[df['cv_score'] == df['cv_score'].min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";\"Sequential(\n",
    "  (0): Linear(in_features=12, out_features=10, bias=True)\n",
    "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
    "  (sigmoid2): Sigmoid()\n",
    "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
    ")\";SGD;0.01;0.0001;5;500;0.0034930747002363204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear( 12, 10), nn.Linear(10, 2), nn.Sigmoid(), nn.Linear(2, 1))\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_name = 'SGD'\n",
    "lr = 0.01\n",
    "weight_decay = 0.0001\n",
    "n_epochs = 500\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset trainable parameters of layer = Linear(in_features=12, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=2, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=1, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 46/46 [00:00<00:00, 380.06batch/s, mse=0.863]\n",
      "Epoch 1: 100%|██████████| 46/46 [00:00<00:00, 440.38batch/s, mse=0.00797]\n",
      "Epoch 2: 100%|██████████| 46/46 [00:00<00:00, 428.07batch/s, mse=0.000146]\n",
      "Epoch 3: 100%|██████████| 46/46 [00:00<00:00, 396.14batch/s, mse=0.000186]\n",
      "Epoch 4: 100%|██████████| 46/46 [00:00<00:00, 603.13batch/s, mse=0.000381]\n",
      "Epoch 5: 100%|██████████| 46/46 [00:00<00:00, 377.77batch/s, mse=0.000569]\n",
      "Epoch 6: 100%|██████████| 46/46 [00:00<00:00, 380.06batch/s, mse=0.000719]\n",
      "Epoch 7: 100%|██████████| 46/46 [00:00<00:00, 471.42batch/s, mse=0.000833]\n",
      "Epoch 8: 100%|██████████| 46/46 [00:00<00:00, 403.85batch/s, mse=0.000918]\n",
      "Epoch 9: 100%|██████████| 46/46 [00:00<00:00, 425.68batch/s, mse=0.000981]\n",
      "Epoch 10: 100%|██████████| 46/46 [00:00<00:00, 527.66batch/s, mse=0.00103] \n",
      "Epoch 11: 100%|██████████| 46/46 [00:00<00:00, 354.47batch/s, mse=0.00106] \n",
      "Epoch 12: 100%|██████████| 46/46 [00:00<00:00, 537.85batch/s, mse=0.00109] \n",
      "Epoch 13: 100%|██████████| 46/46 [00:00<00:00, 521.57batch/s, mse=0.0011]  \n",
      "Epoch 14: 100%|██████████| 46/46 [00:00<00:00, 405.32batch/s, mse=0.00111] \n",
      "Epoch 15: 100%|██████████| 46/46 [00:00<00:00, 529.44batch/s, mse=0.00112] \n",
      "Epoch 16: 100%|██████████| 46/46 [00:00<00:00, 513.31batch/s, mse=0.00112] \n",
      "Epoch 17: 100%|██████████| 46/46 [00:00<00:00, 398.09batch/s, mse=0.00113] \n",
      "Epoch 18: 100%|██████████| 46/46 [00:00<00:00, 466.22batch/s, mse=0.00112] \n",
      "Epoch 19: 100%|██████████| 46/46 [00:00<00:00, 495.57batch/s, mse=0.00112] \n",
      "Epoch 20: 100%|██████████| 46/46 [00:00<00:00, 396.76batch/s, mse=0.00112] \n",
      "Epoch 21: 100%|██████████| 46/46 [00:00<00:00, 446.65batch/s, mse=0.00111] \n",
      "Epoch 22: 100%|██████████| 46/46 [00:00<00:00, 493.35batch/s, mse=0.0011]  \n",
      "Epoch 23: 100%|██████████| 46/46 [00:00<00:00, 405.14batch/s, mse=0.0011]  \n",
      "Epoch 24: 100%|██████████| 46/46 [00:00<00:00, 513.45batch/s, mse=0.00109] \n",
      "Epoch 25: 100%|██████████| 46/46 [00:00<00:00, 419.77batch/s, mse=0.00108] \n",
      "Epoch 26: 100%|██████████| 46/46 [00:00<00:00, 513.80batch/s, mse=0.00107] \n",
      "Epoch 27: 100%|██████████| 46/46 [00:00<00:00, 434.17batch/s, mse=0.00106] \n",
      "Epoch 28: 100%|██████████| 46/46 [00:00<00:00, 490.91batch/s, mse=0.00106] \n",
      "Epoch 29: 100%|██████████| 46/46 [00:00<00:00, 479.32batch/s, mse=0.00105] \n",
      "Epoch 30: 100%|██████████| 46/46 [00:00<00:00, 390.07batch/s, mse=0.00104] \n",
      "Epoch 31: 100%|██████████| 46/46 [00:00<00:00, 523.88batch/s, mse=0.00103] \n",
      "Epoch 32: 100%|██████████| 46/46 [00:00<00:00, 400.87batch/s, mse=0.00103] \n",
      "Epoch 33: 100%|██████████| 46/46 [00:00<00:00, 533.22batch/s, mse=0.00102] \n",
      "Epoch 34: 100%|██████████| 46/46 [00:00<00:00, 511.11batch/s, mse=0.00101] \n",
      "Epoch 35: 100%|██████████| 46/46 [00:00<00:00, 405.10batch/s, mse=0.00101] \n",
      "Epoch 36: 100%|██████████| 46/46 [00:00<00:00, 533.17batch/s, mse=0.001]   \n",
      "Epoch 37: 100%|██████████| 46/46 [00:00<00:00, 483.10batch/s, mse=0.000994]\n",
      "Epoch 38: 100%|██████████| 46/46 [00:00<00:00, 432.89batch/s, mse=0.000988]\n",
      "Epoch 39: 100%|██████████| 46/46 [00:00<00:00, 550.96batch/s, mse=0.000982]\n",
      "Epoch 40: 100%|██████████| 46/46 [00:00<00:00, 394.21batch/s, mse=0.000977]\n",
      "Epoch 41: 100%|██████████| 46/46 [00:00<00:00, 530.27batch/s, mse=0.000972]\n",
      "Epoch 42: 100%|██████████| 46/46 [00:00<00:00, 537.21batch/s, mse=0.000967]\n",
      "Epoch 43: 100%|██████████| 46/46 [00:00<00:00, 338.20batch/s, mse=0.000962]\n",
      "Epoch 44: 100%|██████████| 46/46 [00:00<00:00, 490.84batch/s, mse=0.000957]\n",
      "Epoch 45: 100%|██████████| 46/46 [00:00<00:00, 410.78batch/s, mse=0.000953]\n",
      "Epoch 46: 100%|██████████| 46/46 [00:00<00:00, 537.35batch/s, mse=0.000949]\n",
      "Epoch 47: 100%|██████████| 46/46 [00:00<00:00, 543.42batch/s, mse=0.000944]\n",
      "Epoch 48: 100%|██████████| 46/46 [00:00<00:00, 387.62batch/s, mse=0.00094] \n",
      "Epoch 49: 100%|██████████| 46/46 [00:00<00:00, 531.27batch/s, mse=0.000937]\n",
      "Epoch 50: 100%|██████████| 46/46 [00:00<00:00, 544.12batch/s, mse=0.000933]\n",
      "Epoch 51: 100%|██████████| 46/46 [00:00<00:00, 403.37batch/s, mse=0.000929]\n",
      "Epoch 52: 100%|██████████| 46/46 [00:00<00:00, 519.59batch/s, mse=0.000926]\n",
      "Epoch 53: 100%|██████████| 46/46 [00:00<00:00, 536.48batch/s, mse=0.000923]\n",
      "Epoch 54: 100%|██████████| 46/46 [00:00<00:00, 402.17batch/s, mse=0.00092] \n",
      "Epoch 55: 100%|██████████| 46/46 [00:00<00:00, 524.22batch/s, mse=0.000917]\n",
      "Epoch 56: 100%|██████████| 46/46 [00:00<00:00, 531.46batch/s, mse=0.000914]\n",
      "Epoch 57: 100%|██████████| 46/46 [00:00<00:00, 385.08batch/s, mse=0.000911]\n",
      "Epoch 58: 100%|██████████| 46/46 [00:00<00:00, 518.66batch/s, mse=0.000908]\n",
      "Epoch 59: 100%|██████████| 46/46 [00:00<00:00, 500.69batch/s, mse=0.000906]\n",
      "Epoch 60: 100%|██████████| 46/46 [00:00<00:00, 403.76batch/s, mse=0.000903]\n",
      "Epoch 61: 100%|██████████| 46/46 [00:00<00:00, 518.97batch/s, mse=0.000901]\n",
      "Epoch 62: 100%|██████████| 46/46 [00:00<00:00, 443.71batch/s, mse=0.000899]\n",
      "Epoch 63: 100%|██████████| 46/46 [00:00<00:00, 412.06batch/s, mse=0.000896]\n",
      "Epoch 64: 100%|██████████| 46/46 [00:00<00:00, 476.10batch/s, mse=0.000894]\n",
      "Epoch 65: 100%|██████████| 46/46 [00:00<00:00, 345.94batch/s, mse=0.000892]\n",
      "Epoch 66: 100%|██████████| 46/46 [00:00<00:00, 491.75batch/s, mse=0.00089] \n",
      "Epoch 67: 100%|██████████| 46/46 [00:00<00:00, 508.02batch/s, mse=0.000888]\n",
      "Epoch 68: 100%|██████████| 46/46 [00:00<00:00, 398.00batch/s, mse=0.000886]\n",
      "Epoch 69: 100%|██████████| 46/46 [00:00<00:00, 453.20batch/s, mse=0.000884]\n",
      "Epoch 70: 100%|██████████| 46/46 [00:00<00:00, 517.76batch/s, mse=0.000882]\n",
      "Epoch 71: 100%|██████████| 46/46 [00:00<00:00, 391.26batch/s, mse=0.000881]\n",
      "Epoch 72: 100%|██████████| 46/46 [00:00<00:00, 543.17batch/s, mse=0.000879]\n",
      "Epoch 73: 100%|██████████| 46/46 [00:00<00:00, 443.91batch/s, mse=0.000877]\n",
      "Epoch 74: 100%|██████████| 46/46 [00:00<00:00, 361.14batch/s, mse=0.000876]\n",
      "Epoch 75: 100%|██████████| 46/46 [00:00<00:00, 432.22batch/s, mse=0.000874]\n",
      "Epoch 76: 100%|██████████| 46/46 [00:00<00:00, 355.56batch/s, mse=0.000873]\n",
      "Epoch 77: 100%|██████████| 46/46 [00:00<00:00, 401.14batch/s, mse=0.000871]\n",
      "Epoch 78: 100%|██████████| 46/46 [00:00<00:00, 346.97batch/s, mse=0.000869]\n",
      "Epoch 79: 100%|██████████| 46/46 [00:00<00:00, 397.15batch/s, mse=0.000868]\n",
      "Epoch 80: 100%|██████████| 46/46 [00:00<00:00, 525.67batch/s, mse=0.000867]\n",
      "Epoch 81: 100%|██████████| 46/46 [00:00<00:00, 404.73batch/s, mse=0.000865]\n",
      "Epoch 82: 100%|██████████| 46/46 [00:00<00:00, 431.32batch/s, mse=0.000864]\n",
      "Epoch 83: 100%|██████████| 46/46 [00:00<00:00, 543.16batch/s, mse=0.000863]\n",
      "Epoch 84: 100%|██████████| 46/46 [00:00<00:00, 398.16batch/s, mse=0.000861]\n",
      "Epoch 85: 100%|██████████| 46/46 [00:00<00:00, 513.19batch/s, mse=0.00086] \n",
      "Epoch 86: 100%|██████████| 46/46 [00:00<00:00, 497.46batch/s, mse=0.000859]\n",
      "Epoch 87: 100%|██████████| 46/46 [00:00<00:00, 412.30batch/s, mse=0.000858]\n",
      "Epoch 88: 100%|██████████| 46/46 [00:00<00:00, 538.33batch/s, mse=0.000857]\n",
      "Epoch 89: 100%|██████████| 46/46 [00:00<00:00, 378.93batch/s, mse=0.000856]\n",
      "Epoch 90: 100%|██████████| 46/46 [00:00<00:00, 442.07batch/s, mse=0.000854]\n",
      "Epoch 91: 100%|██████████| 46/46 [00:00<00:00, 494.36batch/s, mse=0.000853]\n",
      "Epoch 92: 100%|██████████| 46/46 [00:00<00:00, 367.67batch/s, mse=0.000852]\n",
      "Epoch 93: 100%|██████████| 46/46 [00:00<00:00, 487.43batch/s, mse=0.000851]\n",
      "Epoch 94: 100%|██████████| 46/46 [00:00<00:00, 391.31batch/s, mse=0.00085] \n",
      "Epoch 95: 100%|██████████| 46/46 [00:00<00:00, 501.58batch/s, mse=0.000849]\n",
      "Epoch 96: 100%|██████████| 46/46 [00:00<00:00, 513.21batch/s, mse=0.000848]\n",
      "Epoch 97: 100%|██████████| 46/46 [00:00<00:00, 399.57batch/s, mse=0.000847]\n",
      "Epoch 98: 100%|██████████| 46/46 [00:00<00:00, 536.87batch/s, mse=0.000846]\n",
      "Epoch 99: 100%|██████████| 46/46 [00:00<00:00, 417.65batch/s, mse=0.000845]\n",
      "Epoch 100: 100%|██████████| 46/46 [00:00<00:00, 512.77batch/s, mse=0.000844]\n",
      "Epoch 101: 100%|██████████| 46/46 [00:00<00:00, 475.97batch/s, mse=0.000843]\n",
      "Epoch 102: 100%|██████████| 46/46 [00:00<00:00, 342.17batch/s, mse=0.000842]\n",
      "Epoch 103: 100%|██████████| 46/46 [00:00<00:00, 466.78batch/s, mse=0.000842]\n",
      "Epoch 104: 100%|██████████| 46/46 [00:00<00:00, 392.70batch/s, mse=0.000841]\n",
      "Epoch 105: 100%|██████████| 46/46 [00:00<00:00, 376.74batch/s, mse=0.00084] \n",
      "Epoch 106: 100%|██████████| 46/46 [00:00<00:00, 514.46batch/s, mse=0.000839]\n",
      "Epoch 107: 100%|██████████| 46/46 [00:00<00:00, 375.01batch/s, mse=0.000838]\n",
      "Epoch 108: 100%|██████████| 46/46 [00:00<00:00, 536.26batch/s, mse=0.000837]\n",
      "Epoch 109: 100%|██████████| 46/46 [00:00<00:00, 440.25batch/s, mse=0.000837]\n",
      "Epoch 110: 100%|██████████| 46/46 [00:00<00:00, 448.47batch/s, mse=0.000836]\n",
      "Epoch 111: 100%|██████████| 46/46 [00:00<00:00, 518.24batch/s, mse=0.000835]\n",
      "Epoch 112: 100%|██████████| 46/46 [00:00<00:00, 387.19batch/s, mse=0.000834]\n",
      "Epoch 113: 100%|██████████| 46/46 [00:00<00:00, 486.59batch/s, mse=0.000833]\n",
      "Epoch 114: 100%|██████████| 46/46 [00:00<00:00, 427.29batch/s, mse=0.000833]\n",
      "Epoch 115: 100%|██████████| 46/46 [00:00<00:00, 451.92batch/s, mse=0.000832]\n",
      "Epoch 116: 100%|██████████| 46/46 [00:00<00:00, 543.94batch/s, mse=0.000831]\n",
      "Epoch 117: 100%|██████████| 46/46 [00:00<00:00, 490.33batch/s, mse=0.00083] \n",
      "Epoch 118: 100%|██████████| 46/46 [00:00<00:00, 375.50batch/s, mse=0.00083] \n",
      "Epoch 119: 100%|██████████| 46/46 [00:00<00:00, 475.80batch/s, mse=0.000829]\n",
      "Epoch 120: 100%|██████████| 46/46 [00:00<00:00, 382.11batch/s, mse=0.000828]\n",
      "Epoch 121: 100%|██████████| 46/46 [00:00<00:00, 519.17batch/s, mse=0.000827]\n",
      "Epoch 122: 100%|██████████| 46/46 [00:00<00:00, 417.15batch/s, mse=0.000827]\n",
      "Epoch 123: 100%|██████████| 46/46 [00:00<00:00, 544.22batch/s, mse=0.000826]\n",
      "Epoch 124: 100%|██████████| 46/46 [00:00<00:00, 475.85batch/s, mse=0.000825]\n",
      "Epoch 125: 100%|██████████| 46/46 [00:00<00:00, 501.68batch/s, mse=0.000825]\n",
      "Epoch 126: 100%|██████████| 46/46 [00:00<00:00, 507.28batch/s, mse=0.000824]\n",
      "Epoch 127: 100%|██████████| 46/46 [00:00<00:00, 350.68batch/s, mse=0.000823]\n",
      "Epoch 128: 100%|██████████| 46/46 [00:00<00:00, 476.58batch/s, mse=0.000823]\n",
      "Epoch 129: 100%|██████████| 46/46 [00:00<00:00, 380.14batch/s, mse=0.000822]\n",
      "Epoch 130: 100%|██████████| 46/46 [00:00<00:00, 489.11batch/s, mse=0.000821]\n",
      "Epoch 131: 100%|██████████| 46/46 [00:00<00:00, 408.23batch/s, mse=0.000821]\n",
      "Epoch 132: 100%|██████████| 46/46 [00:00<00:00, 550.38batch/s, mse=0.00082] \n",
      "Epoch 133: 100%|██████████| 46/46 [00:00<00:00, 466.95batch/s, mse=0.00082] \n",
      "Epoch 134: 100%|██████████| 46/46 [00:00<00:00, 472.54batch/s, mse=0.000819]\n",
      "Epoch 135: 100%|██████████| 46/46 [00:00<00:00, 553.62batch/s, mse=0.000818]\n",
      "Epoch 136: 100%|██████████| 46/46 [00:00<00:00, 399.29batch/s, mse=0.000818]\n",
      "Epoch 137: 100%|██████████| 46/46 [00:00<00:00, 505.83batch/s, mse=0.000817]\n",
      "Epoch 138: 100%|██████████| 46/46 [00:00<00:00, 419.58batch/s, mse=0.000817]\n",
      "Epoch 139: 100%|██████████| 46/46 [00:00<00:00, 540.62batch/s, mse=0.000816]\n",
      "Epoch 140: 100%|██████████| 46/46 [00:00<00:00, 522.90batch/s, mse=0.000815]\n",
      "Epoch 141: 100%|██████████| 46/46 [00:00<00:00, 502.37batch/s, mse=0.000815]\n",
      "Epoch 142: 100%|██████████| 46/46 [00:00<00:00, 508.17batch/s, mse=0.000814]\n",
      "Epoch 143: 100%|██████████| 46/46 [00:00<00:00, 390.36batch/s, mse=0.000814]\n",
      "Epoch 144: 100%|██████████| 46/46 [00:00<00:00, 568.13batch/s, mse=0.000813]\n",
      "Epoch 145: 100%|██████████| 46/46 [00:00<00:00, 427.07batch/s, mse=0.000813]\n",
      "Epoch 146: 100%|██████████| 46/46 [00:00<00:00, 416.01batch/s, mse=0.000812]\n",
      "Epoch 147: 100%|██████████| 46/46 [00:00<00:00, 535.91batch/s, mse=0.000811]\n",
      "Epoch 148: 100%|██████████| 46/46 [00:00<00:00, 372.75batch/s, mse=0.000811]\n",
      "Epoch 149: 100%|██████████| 46/46 [00:00<00:00, 504.55batch/s, mse=0.00081] \n",
      "Epoch 150: 100%|██████████| 46/46 [00:00<00:00, 375.33batch/s, mse=0.00081] \n",
      "Epoch 151: 100%|██████████| 46/46 [00:00<00:00, 518.87batch/s, mse=0.000809]\n",
      "Epoch 152: 100%|██████████| 46/46 [00:00<00:00, 405.01batch/s, mse=0.000809]\n",
      "Epoch 153: 100%|██████████| 46/46 [00:00<00:00, 549.08batch/s, mse=0.000808]\n",
      "Epoch 154: 100%|██████████| 46/46 [00:00<00:00, 465.33batch/s, mse=0.000808]\n",
      "Epoch 155: 100%|██████████| 46/46 [00:00<00:00, 452.09batch/s, mse=0.000807]\n",
      "Epoch 156: 100%|██████████| 46/46 [00:00<00:00, 550.04batch/s, mse=0.000807]\n",
      "Epoch 157: 100%|██████████| 46/46 [00:00<00:00, 382.84batch/s, mse=0.000806]\n",
      "Epoch 158: 100%|██████████| 46/46 [00:00<00:00, 525.70batch/s, mse=0.000806]\n",
      "Epoch 159: 100%|██████████| 46/46 [00:00<00:00, 461.32batch/s, mse=0.000805]\n",
      "Epoch 160: 100%|██████████| 46/46 [00:00<00:00, 514.78batch/s, mse=0.000805]\n",
      "Epoch 161: 100%|██████████| 46/46 [00:00<00:00, 563.59batch/s, mse=0.000804]\n",
      "Epoch 162: 100%|██████████| 46/46 [00:00<00:00, 391.39batch/s, mse=0.000804]\n",
      "Epoch 163: 100%|██████████| 46/46 [00:00<00:00, 501.71batch/s, mse=0.000803]\n",
      "Epoch 164: 100%|██████████| 46/46 [00:00<00:00, 408.35batch/s, mse=0.000803]\n",
      "Epoch 165: 100%|██████████| 46/46 [00:00<00:00, 567.08batch/s, mse=0.000802]\n",
      "Epoch 166: 100%|██████████| 46/46 [00:00<00:00, 408.92batch/s, mse=0.000802]\n",
      "Epoch 167: 100%|██████████| 46/46 [00:00<00:00, 550.01batch/s, mse=0.000801]\n",
      "Epoch 168: 100%|██████████| 46/46 [00:00<00:00, 419.37batch/s, mse=0.000801]\n",
      "Epoch 169: 100%|██████████| 46/46 [00:00<00:00, 550.13batch/s, mse=0.0008]  \n",
      "Epoch 170: 100%|██████████| 46/46 [00:00<00:00, 514.10batch/s, mse=0.0008]  \n",
      "Epoch 171: 100%|██████████| 46/46 [00:00<00:00, 375.37batch/s, mse=0.000799]\n",
      "Epoch 172: 100%|██████████| 46/46 [00:00<00:00, 550.45batch/s, mse=0.000799]\n",
      "Epoch 173: 100%|██████████| 46/46 [00:00<00:00, 404.61batch/s, mse=0.000798]\n",
      "Epoch 174: 100%|██████████| 46/46 [00:00<00:00, 576.85batch/s, mse=0.000798]\n",
      "Epoch 175: 100%|██████████| 46/46 [00:00<00:00, 407.83batch/s, mse=0.000798]\n",
      "Epoch 176: 100%|██████████| 46/46 [00:00<00:00, 556.15batch/s, mse=0.000797]\n",
      "Epoch 177: 100%|██████████| 46/46 [00:00<00:00, 456.63batch/s, mse=0.000797]\n",
      "Epoch 178: 100%|██████████| 46/46 [00:00<00:00, 486.07batch/s, mse=0.000796]\n",
      "Epoch 179: 100%|██████████| 46/46 [00:00<00:00, 578.72batch/s, mse=0.000796]\n",
      "Epoch 180: 100%|██████████| 46/46 [00:00<00:00, 383.26batch/s, mse=0.000795]\n",
      "Epoch 181: 100%|██████████| 46/46 [00:00<00:00, 461.94batch/s, mse=0.000795]\n",
      "Epoch 182: 100%|██████████| 46/46 [00:00<00:00, 462.16batch/s, mse=0.000794]\n",
      "Epoch 183: 100%|██████████| 46/46 [00:00<00:00, 506.08batch/s, mse=0.000794]\n",
      "Epoch 184: 100%|██████████| 46/46 [00:00<00:00, 421.09batch/s, mse=0.000794]\n",
      "Epoch 185: 100%|██████████| 46/46 [00:00<00:00, 556.26batch/s, mse=0.000793]\n",
      "Epoch 186: 100%|██████████| 46/46 [00:00<00:00, 399.72batch/s, mse=0.000793]\n",
      "Epoch 187: 100%|██████████| 46/46 [00:00<00:00, 556.28batch/s, mse=0.000792]\n",
      "Epoch 188: 100%|██████████| 46/46 [00:00<00:00, 407.24batch/s, mse=0.000792]\n",
      "Epoch 189: 100%|██████████| 46/46 [00:00<00:00, 585.39batch/s, mse=0.000791]\n",
      "Epoch 190: 100%|██████████| 46/46 [00:00<00:00, 423.17batch/s, mse=0.000791]\n",
      "Epoch 191: 100%|██████████| 46/46 [00:00<00:00, 547.89batch/s, mse=0.000791]\n",
      "Epoch 192: 100%|██████████| 46/46 [00:00<00:00, 412.64batch/s, mse=0.00079] \n",
      "Epoch 193: 100%|██████████| 46/46 [00:00<00:00, 549.98batch/s, mse=0.00079] \n",
      "Epoch 194: 100%|██████████| 46/46 [00:00<00:00, 423.43batch/s, mse=0.000789]\n",
      "Epoch 195: 100%|██████████| 46/46 [00:00<00:00, 548.81batch/s, mse=0.000789]\n",
      "Epoch 196: 100%|██████████| 46/46 [00:00<00:00, 384.79batch/s, mse=0.000789]\n",
      "Epoch 197: 100%|██████████| 46/46 [00:00<00:00, 485.71batch/s, mse=0.000788]\n",
      "Epoch 198: 100%|██████████| 46/46 [00:00<00:00, 452.76batch/s, mse=0.000788]\n",
      "Epoch 199: 100%|██████████| 46/46 [00:00<00:00, 501.53batch/s, mse=0.000787]\n",
      "Epoch 200: 100%|██████████| 46/46 [00:00<00:00, 556.47batch/s, mse=0.000787]\n",
      "Epoch 201: 100%|██████████| 46/46 [00:00<00:00, 419.63batch/s, mse=0.000787]\n",
      "Epoch 202: 100%|██████████| 46/46 [00:00<00:00, 556.37batch/s, mse=0.000786]\n",
      "Epoch 203: 100%|██████████| 46/46 [00:00<00:00, 407.71batch/s, mse=0.000786]\n",
      "Epoch 204: 100%|██████████| 46/46 [00:00<00:00, 569.84batch/s, mse=0.000785]\n",
      "Epoch 205: 100%|██████████| 46/46 [00:00<00:00, 401.21batch/s, mse=0.000785]\n",
      "Epoch 206: 100%|██████████| 46/46 [00:00<00:00, 538.92batch/s, mse=0.000785]\n",
      "Epoch 207: 100%|██████████| 46/46 [00:00<00:00, 356.66batch/s, mse=0.000784]\n",
      "Epoch 208: 100%|██████████| 46/46 [00:00<00:00, 454.98batch/s, mse=0.000784]\n",
      "Epoch 209: 100%|██████████| 46/46 [00:00<00:00, 389.22batch/s, mse=0.000783]\n",
      "Epoch 210: 100%|██████████| 46/46 [00:00<00:00, 536.21batch/s, mse=0.000783]\n",
      "Epoch 211: 100%|██████████| 46/46 [00:00<00:00, 369.10batch/s, mse=0.000783]\n",
      "Epoch 212: 100%|██████████| 46/46 [00:00<00:00, 563.53batch/s, mse=0.000782]\n",
      "Epoch 213: 100%|██████████| 46/46 [00:00<00:00, 363.23batch/s, mse=0.000782]\n",
      "Epoch 214: 100%|██████████| 46/46 [00:00<00:00, 373.07batch/s, mse=0.000782]\n",
      "Epoch 215: 100%|██████████| 46/46 [00:00<00:00, 386.85batch/s, mse=0.000781]\n",
      "Epoch 216: 100%|██████████| 46/46 [00:00<00:00, 336.08batch/s, mse=0.000781]\n",
      "Epoch 217: 100%|██████████| 46/46 [00:00<00:00, 397.08batch/s, mse=0.00078] \n",
      "Epoch 218: 100%|██████████| 46/46 [00:00<00:00, 368.46batch/s, mse=0.00078] \n",
      "Epoch 219: 100%|██████████| 46/46 [00:00<00:00, 419.20batch/s, mse=0.00078]\n",
      "Epoch 220: 100%|██████████| 46/46 [00:00<00:00, 382.22batch/s, mse=0.000779]\n",
      "Epoch 221: 100%|██████████| 46/46 [00:00<00:00, 461.47batch/s, mse=0.000779]\n",
      "Epoch 222: 100%|██████████| 46/46 [00:00<00:00, 419.57batch/s, mse=0.000779]\n",
      "Epoch 223: 100%|██████████| 46/46 [00:00<00:00, 457.26batch/s, mse=0.000778]\n",
      "Epoch 224: 100%|██████████| 46/46 [00:00<00:00, 427.82batch/s, mse=0.000778]\n",
      "Epoch 225: 100%|██████████| 46/46 [00:00<00:00, 354.38batch/s, mse=0.000777]\n",
      "Epoch 226: 100%|██████████| 46/46 [00:00<00:00, 439.63batch/s, mse=0.000777]\n",
      "Epoch 227: 100%|██████████| 46/46 [00:00<00:00, 448.37batch/s, mse=0.000777]\n",
      "Epoch 228: 100%|██████████| 46/46 [00:00<00:00, 473.03batch/s, mse=0.000776]\n",
      "Epoch 229: 100%|██████████| 46/46 [00:00<00:00, 411.14batch/s, mse=0.000776]\n",
      "Epoch 230: 100%|██████████| 46/46 [00:00<00:00, 573.86batch/s, mse=0.000776]\n",
      "Epoch 231: 100%|██████████| 46/46 [00:00<00:00, 388.16batch/s, mse=0.000775]\n",
      "Epoch 232: 100%|██████████| 46/46 [00:00<00:00, 495.09batch/s, mse=0.000775]\n",
      "Epoch 233: 100%|██████████| 46/46 [00:00<00:00, 401.05batch/s, mse=0.000775]\n",
      "Epoch 234: 100%|██████████| 46/46 [00:00<00:00, 526.74batch/s, mse=0.000774]\n",
      "Epoch 235: 100%|██████████| 46/46 [00:00<00:00, 394.02batch/s, mse=0.000774]\n",
      "Epoch 236: 100%|██████████| 46/46 [00:00<00:00, 504.89batch/s, mse=0.000774]\n",
      "Epoch 237: 100%|██████████| 46/46 [00:00<00:00, 381.93batch/s, mse=0.000773]\n",
      "Epoch 238: 100%|██████████| 46/46 [00:00<00:00, 531.31batch/s, mse=0.000773]\n",
      "Epoch 239: 100%|██████████| 46/46 [00:00<00:00, 367.40batch/s, mse=0.000773]\n",
      "Epoch 240: 100%|██████████| 46/46 [00:00<00:00, 504.91batch/s, mse=0.000772]\n",
      "Epoch 241: 100%|██████████| 46/46 [00:00<00:00, 375.61batch/s, mse=0.000772]\n",
      "Epoch 242: 100%|██████████| 46/46 [00:00<00:00, 481.37batch/s, mse=0.000772]\n",
      "Epoch 243: 100%|██████████| 46/46 [00:00<00:00, 339.15batch/s, mse=0.000771]\n",
      "Epoch 244: 100%|██████████| 46/46 [00:00<00:00, 502.76batch/s, mse=0.000771]\n",
      "Epoch 245: 100%|██████████| 46/46 [00:00<00:00, 389.94batch/s, mse=0.000771]\n",
      "Epoch 246: 100%|██████████| 46/46 [00:00<00:00, 496.62batch/s, mse=0.00077] \n",
      "Epoch 247: 100%|██████████| 46/46 [00:00<00:00, 386.53batch/s, mse=0.00077] \n",
      "Epoch 248: 100%|██████████| 46/46 [00:00<00:00, 496.15batch/s, mse=0.00077] \n",
      "Epoch 249: 100%|██████████| 46/46 [00:00<00:00, 388.82batch/s, mse=0.000769]\n",
      "Epoch 250: 100%|██████████| 46/46 [00:00<00:00, 486.48batch/s, mse=0.000769]\n",
      "Epoch 251: 100%|██████████| 46/46 [00:00<00:00, 361.79batch/s, mse=0.000769]\n",
      "Epoch 252: 100%|██████████| 46/46 [00:00<00:00, 512.85batch/s, mse=0.000768]\n",
      "Epoch 253: 100%|██████████| 46/46 [00:00<00:00, 371.63batch/s, mse=0.000768]\n",
      "Epoch 254: 100%|██████████| 46/46 [00:00<00:00, 501.75batch/s, mse=0.000768]\n",
      "Epoch 255: 100%|██████████| 46/46 [00:00<00:00, 384.80batch/s, mse=0.000767]\n",
      "Epoch 256: 100%|██████████| 46/46 [00:00<00:00, 496.45batch/s, mse=0.000767]\n",
      "Epoch 257: 100%|██████████| 46/46 [00:00<00:00, 396.54batch/s, mse=0.000767]\n",
      "Epoch 258: 100%|██████████| 46/46 [00:00<00:00, 525.35batch/s, mse=0.000766]\n",
      "Epoch 259: 100%|██████████| 46/46 [00:00<00:00, 309.50batch/s, mse=0.000766]\n",
      "Epoch 260: 100%|██████████| 46/46 [00:00<00:00, 488.40batch/s, mse=0.000766]\n",
      "Epoch 261: 100%|██████████| 46/46 [00:00<00:00, 408.61batch/s, mse=0.000765]\n",
      "Epoch 262: 100%|██████████| 46/46 [00:00<00:00, 435.75batch/s, mse=0.000765]\n",
      "Epoch 263: 100%|██████████| 46/46 [00:00<00:00, 416.02batch/s, mse=0.000765]\n",
      "Epoch 264: 100%|██████████| 46/46 [00:00<00:00, 444.19batch/s, mse=0.000764]\n",
      "Epoch 265: 100%|██████████| 46/46 [00:00<00:00, 444.28batch/s, mse=0.000764]\n",
      "Epoch 266: 100%|██████████| 46/46 [00:00<00:00, 425.04batch/s, mse=0.000764]\n",
      "Epoch 267: 100%|██████████| 46/46 [00:00<00:00, 456.90batch/s, mse=0.000764]\n",
      "Epoch 268: 100%|██████████| 46/46 [00:00<00:00, 415.05batch/s, mse=0.000763]\n",
      "Epoch 269: 100%|██████████| 46/46 [00:00<00:00, 481.47batch/s, mse=0.000763]\n",
      "Epoch 270: 100%|██████████| 46/46 [00:00<00:00, 387.68batch/s, mse=0.000763]\n",
      "Epoch 271: 100%|██████████| 46/46 [00:00<00:00, 483.97batch/s, mse=0.000762]\n",
      "Epoch 272: 100%|██████████| 46/46 [00:00<00:00, 361.65batch/s, mse=0.000762]\n",
      "Epoch 273: 100%|██████████| 46/46 [00:00<00:00, 418.53batch/s, mse=0.000762]\n",
      "Epoch 274: 100%|██████████| 46/46 [00:00<00:00, 331.87batch/s, mse=0.000761]\n",
      "Epoch 275: 100%|██████████| 46/46 [00:00<00:00, 352.28batch/s, mse=0.000761]\n",
      "Epoch 276: 100%|██████████| 46/46 [00:00<00:00, 351.47batch/s, mse=0.000761]\n",
      "Epoch 277: 100%|██████████| 46/46 [00:00<00:00, 381.49batch/s, mse=0.000761]\n",
      "Epoch 278: 100%|██████████| 46/46 [00:00<00:00, 380.58batch/s, mse=0.00076] \n",
      "Epoch 279: 100%|██████████| 46/46 [00:00<00:00, 370.84batch/s, mse=0.00076] \n",
      "Epoch 280: 100%|██████████| 46/46 [00:00<00:00, 403.72batch/s, mse=0.00076] \n",
      "Epoch 281: 100%|██████████| 46/46 [00:00<00:00, 338.34batch/s, mse=0.000759]\n",
      "Epoch 282: 100%|██████████| 46/46 [00:00<00:00, 419.75batch/s, mse=0.000759]\n",
      "Epoch 283: 100%|██████████| 46/46 [00:00<00:00, 343.52batch/s, mse=0.000759]\n",
      "Epoch 284: 100%|██████████| 46/46 [00:00<00:00, 412.04batch/s, mse=0.000759]\n",
      "Epoch 285: 100%|██████████| 46/46 [00:00<00:00, 350.22batch/s, mse=0.000758]\n",
      "Epoch 286: 100%|██████████| 46/46 [00:00<00:00, 439.62batch/s, mse=0.000758]\n",
      "Epoch 287: 100%|██████████| 46/46 [00:00<00:00, 345.22batch/s, mse=0.000758]\n",
      "Epoch 288: 100%|██████████| 46/46 [00:00<00:00, 404.31batch/s, mse=0.000757]\n",
      "Epoch 289: 100%|██████████| 46/46 [00:00<00:00, 341.31batch/s, mse=0.000757]\n",
      "Epoch 290: 100%|██████████| 46/46 [00:00<00:00, 391.72batch/s, mse=0.000757]\n",
      "Epoch 291: 100%|██████████| 46/46 [00:00<00:00, 359.69batch/s, mse=0.000756]\n",
      "Epoch 292: 100%|██████████| 46/46 [00:00<00:00, 453.95batch/s, mse=0.000756]\n",
      "Epoch 293: 100%|██████████| 46/46 [00:00<00:00, 447.00batch/s, mse=0.000756]\n",
      "Epoch 294: 100%|██████████| 46/46 [00:00<00:00, 430.64batch/s, mse=0.000756]\n",
      "Epoch 295: 100%|██████████| 46/46 [00:00<00:00, 446.68batch/s, mse=0.000755]\n",
      "Epoch 296: 100%|██████████| 46/46 [00:00<00:00, 421.56batch/s, mse=0.000755]\n",
      "Epoch 297: 100%|██████████| 46/46 [00:00<00:00, 487.30batch/s, mse=0.000755]\n",
      "Epoch 298: 100%|██████████| 46/46 [00:00<00:00, 387.83batch/s, mse=0.000755]\n",
      "Epoch 299: 100%|██████████| 46/46 [00:00<00:00, 455.69batch/s, mse=0.000754]\n",
      "Epoch 300: 100%|██████████| 46/46 [00:00<00:00, 406.45batch/s, mse=0.000754]\n",
      "Epoch 301: 100%|██████████| 46/46 [00:00<00:00, 496.89batch/s, mse=0.000754]\n",
      "Epoch 302: 100%|██████████| 46/46 [00:00<00:00, 394.92batch/s, mse=0.000753]\n",
      "Epoch 303: 100%|██████████| 46/46 [00:00<00:00, 377.64batch/s, mse=0.000753]\n",
      "Epoch 304: 100%|██████████| 46/46 [00:00<00:00, 352.37batch/s, mse=0.000753]\n",
      "Epoch 305: 100%|██████████| 46/46 [00:00<00:00, 465.61batch/s, mse=0.000753]\n",
      "Epoch 306: 100%|██████████| 46/46 [00:00<00:00, 374.31batch/s, mse=0.000752]\n",
      "Epoch 307: 100%|██████████| 46/46 [00:00<00:00, 513.30batch/s, mse=0.000752]\n",
      "Epoch 308: 100%|██████████| 46/46 [00:00<00:00, 387.55batch/s, mse=0.000752]\n",
      "Epoch 309: 100%|██████████| 46/46 [00:00<00:00, 486.81batch/s, mse=0.000752]\n",
      "Epoch 310: 100%|██████████| 46/46 [00:00<00:00, 367.48batch/s, mse=0.000751]\n",
      "Epoch 311: 100%|██████████| 46/46 [00:00<00:00, 462.25batch/s, mse=0.000751]\n",
      "Epoch 312: 100%|██████████| 46/46 [00:00<00:00, 418.52batch/s, mse=0.000751]\n",
      "Epoch 313: 100%|██████████| 46/46 [00:00<00:00, 469.72batch/s, mse=0.00075] \n",
      "Epoch 314: 100%|██████████| 46/46 [00:00<00:00, 424.79batch/s, mse=0.00075] \n",
      "Epoch 315: 100%|██████████| 46/46 [00:00<00:00, 428.32batch/s, mse=0.00075] \n",
      "Epoch 316: 100%|██████████| 46/46 [00:00<00:00, 361.53batch/s, mse=0.00075] \n",
      "Epoch 317: 100%|██████████| 46/46 [00:00<00:00, 378.37batch/s, mse=0.000749]\n",
      "Epoch 318: 100%|██████████| 46/46 [00:00<00:00, 493.99batch/s, mse=0.000749]\n",
      "Epoch 319: 100%|██████████| 46/46 [00:00<00:00, 384.79batch/s, mse=0.000749]\n",
      "Epoch 320: 100%|██████████| 46/46 [00:00<00:00, 486.49batch/s, mse=0.000749]\n",
      "Epoch 321: 100%|██████████| 46/46 [00:00<00:00, 384.95batch/s, mse=0.000748]\n",
      "Epoch 322: 100%|██████████| 46/46 [00:00<00:00, 501.73batch/s, mse=0.000748]\n",
      "Epoch 323: 100%|██████████| 46/46 [00:00<00:00, 373.84batch/s, mse=0.000748]\n",
      "Epoch 324: 100%|██████████| 46/46 [00:00<00:00, 513.30batch/s, mse=0.000748]\n",
      "Epoch 325: 100%|██████████| 46/46 [00:00<00:00, 380.23batch/s, mse=0.000747]\n",
      "Epoch 326: 100%|██████████| 46/46 [00:00<00:00, 531.18batch/s, mse=0.000747]\n",
      "Epoch 327: 100%|██████████| 46/46 [00:00<00:00, 401.63batch/s, mse=0.000747]\n",
      "Epoch 328: 100%|██████████| 46/46 [00:00<00:00, 443.68batch/s, mse=0.000747]\n",
      "Epoch 329: 100%|██████████| 46/46 [00:00<00:00, 445.94batch/s, mse=0.000746]\n",
      "Epoch 330: 100%|██████████| 46/46 [00:00<00:00, 312.62batch/s, mse=0.000746]\n",
      "Epoch 331: 100%|██████████| 46/46 [00:00<00:00, 499.91batch/s, mse=0.000746]\n",
      "Epoch 332: 100%|██████████| 46/46 [00:00<00:00, 386.00batch/s, mse=0.000746]\n",
      "Epoch 333: 100%|██████████| 46/46 [00:00<00:00, 590.24batch/s, mse=0.000745]\n",
      "Epoch 334: 100%|██████████| 46/46 [00:00<00:00, 379.39batch/s, mse=0.000745]\n",
      "Epoch 335: 100%|██████████| 46/46 [00:00<00:00, 404.40batch/s, mse=0.000745]\n",
      "Epoch 336: 100%|██████████| 46/46 [00:00<00:00, 473.97batch/s, mse=0.000745]\n",
      "Epoch 337: 100%|██████████| 46/46 [00:00<00:00, 344.18batch/s, mse=0.000744]\n",
      "Epoch 338: 100%|██████████| 46/46 [00:00<00:00, 531.48batch/s, mse=0.000744]\n",
      "Epoch 339: 100%|██████████| 46/46 [00:00<00:00, 382.79batch/s, mse=0.000744]\n",
      "Epoch 340: 100%|██████████| 46/46 [00:00<00:00, 518.70batch/s, mse=0.000744]\n",
      "Epoch 341: 100%|██████████| 46/46 [00:00<00:00, 375.18batch/s, mse=0.000743]\n",
      "Epoch 342: 100%|██████████| 46/46 [00:00<00:00, 486.59batch/s, mse=0.000743]\n",
      "Epoch 343: 100%|██████████| 46/46 [00:00<00:00, 312.59batch/s, mse=0.000743]\n",
      "Epoch 344: 100%|██████████| 46/46 [00:00<00:00, 480.98batch/s, mse=0.000743]\n",
      "Epoch 345: 100%|██████████| 46/46 [00:00<00:00, 392.33batch/s, mse=0.000742]\n",
      "Epoch 346: 100%|██████████| 46/46 [00:00<00:00, 397.02batch/s, mse=0.000742]\n",
      "Epoch 347: 100%|██████████| 46/46 [00:00<00:00, 421.84batch/s, mse=0.000742]\n",
      "Epoch 348: 100%|██████████| 46/46 [00:00<00:00, 350.82batch/s, mse=0.000742]\n",
      "Epoch 349: 100%|██████████| 46/46 [00:00<00:00, 453.75batch/s, mse=0.000741]\n",
      "Epoch 350: 100%|██████████| 46/46 [00:00<00:00, 379.47batch/s, mse=0.000741]\n",
      "Epoch 351: 100%|██████████| 46/46 [00:00<00:00, 459.65batch/s, mse=0.000741]\n",
      "Epoch 352: 100%|██████████| 46/46 [00:00<00:00, 382.25batch/s, mse=0.000741]\n",
      "Epoch 353: 100%|██████████| 46/46 [00:00<00:00, 397.14batch/s, mse=0.00074] \n",
      "Epoch 354: 100%|██████████| 46/46 [00:00<00:00, 411.54batch/s, mse=0.00074] \n",
      "Epoch 355: 100%|██████████| 46/46 [00:00<00:00, 317.48batch/s, mse=0.00074] \n",
      "Epoch 356: 100%|██████████| 46/46 [00:00<00:00, 418.69batch/s, mse=0.00074] \n",
      "Epoch 357: 100%|██████████| 46/46 [00:00<00:00, 354.53batch/s, mse=0.00074] \n",
      "Epoch 358: 100%|██████████| 46/46 [00:00<00:00, 525.51batch/s, mse=0.000739]\n",
      "Epoch 359: 100%|██████████| 46/46 [00:00<00:00, 375.59batch/s, mse=0.000739]\n",
      "Epoch 360: 100%|██████████| 46/46 [00:00<00:00, 471.35batch/s, mse=0.000739]\n",
      "Epoch 361: 100%|██████████| 46/46 [00:00<00:00, 420.06batch/s, mse=0.000739]\n",
      "Epoch 362: 100%|██████████| 46/46 [00:00<00:00, 409.27batch/s, mse=0.000738]\n",
      "Epoch 363: 100%|██████████| 46/46 [00:00<00:00, 475.91batch/s, mse=0.000738]\n",
      "Epoch 364: 100%|██████████| 46/46 [00:00<00:00, 385.15batch/s, mse=0.000738]\n",
      "Epoch 365: 100%|██████████| 46/46 [00:00<00:00, 501.80batch/s, mse=0.000738]\n",
      "Epoch 366: 100%|██████████| 46/46 [00:00<00:00, 382.87batch/s, mse=0.000737]\n",
      "Epoch 367: 100%|██████████| 46/46 [00:00<00:00, 485.81batch/s, mse=0.000737]\n",
      "Epoch 368: 100%|██████████| 46/46 [00:00<00:00, 353.34batch/s, mse=0.000737]\n",
      "Epoch 369: 100%|██████████| 46/46 [00:00<00:00, 360.45batch/s, mse=0.000737]\n",
      "Epoch 370: 100%|██████████| 46/46 [00:00<00:00, 452.86batch/s, mse=0.000737]\n",
      "Epoch 371: 100%|██████████| 46/46 [00:00<00:00, 426.42batch/s, mse=0.000736]\n",
      "Epoch 372: 100%|██████████| 46/46 [00:00<00:00, 485.55batch/s, mse=0.000736]\n",
      "Epoch 373: 100%|██████████| 46/46 [00:00<00:00, 356.46batch/s, mse=0.000736]\n",
      "Epoch 374: 100%|██████████| 46/46 [00:00<00:00, 394.53batch/s, mse=0.000736]\n",
      "Epoch 375: 100%|██████████| 46/46 [00:00<00:00, 300.44batch/s, mse=0.000735]\n",
      "Epoch 376: 100%|██████████| 46/46 [00:00<00:00, 388.96batch/s, mse=0.000735]\n",
      "Epoch 377: 100%|██████████| 46/46 [00:00<00:00, 390.60batch/s, mse=0.000735]\n",
      "Epoch 378: 100%|██████████| 46/46 [00:00<00:00, 359.53batch/s, mse=0.000735]\n",
      "Epoch 379: 100%|██████████| 46/46 [00:00<00:00, 468.35batch/s, mse=0.000734]\n",
      "Epoch 380: 100%|██████████| 46/46 [00:00<00:00, 314.64batch/s, mse=0.000734]\n",
      "Epoch 381: 100%|██████████| 46/46 [00:00<00:00, 309.91batch/s, mse=0.000734]\n",
      "Epoch 382: 100%|██████████| 46/46 [00:00<00:00, 306.48batch/s, mse=0.000734]\n",
      "Epoch 383: 100%|██████████| 46/46 [00:00<00:00, 395.49batch/s, mse=0.000734]\n",
      "Epoch 384: 100%|██████████| 46/46 [00:00<00:00, 357.70batch/s, mse=0.000733]\n",
      "Epoch 385: 100%|██████████| 46/46 [00:00<00:00, 387.63batch/s, mse=0.000733]\n",
      "Epoch 386: 100%|██████████| 46/46 [00:00<00:00, 437.07batch/s, mse=0.000733]\n",
      "Epoch 387: 100%|██████████| 46/46 [00:00<00:00, 349.08batch/s, mse=0.000733]\n",
      "Epoch 388: 100%|██████████| 46/46 [00:00<00:00, 443.98batch/s, mse=0.000733]\n",
      "Epoch 389: 100%|██████████| 46/46 [00:00<00:00, 361.68batch/s, mse=0.000732]\n",
      "Epoch 390: 100%|██████████| 46/46 [00:00<00:00, 443.97batch/s, mse=0.000732]\n",
      "Epoch 391: 100%|██████████| 46/46 [00:00<00:00, 358.58batch/s, mse=0.000732]\n",
      "Epoch 392: 100%|██████████| 46/46 [00:00<00:00, 352.89batch/s, mse=0.000732]\n",
      "Epoch 393: 100%|██████████| 46/46 [00:00<00:00, 412.31batch/s, mse=0.000731]\n",
      "Epoch 394: 100%|██████████| 46/46 [00:00<00:00, 393.67batch/s, mse=0.000731]\n",
      "Epoch 395: 100%|██████████| 46/46 [00:00<00:00, 444.67batch/s, mse=0.000731]\n",
      "Epoch 396: 100%|██████████| 46/46 [00:00<00:00, 273.60batch/s, mse=0.000731]\n",
      "Epoch 397: 100%|██████████| 46/46 [00:00<00:00, 365.86batch/s, mse=0.000731]\n",
      "Epoch 398: 100%|██████████| 46/46 [00:00<00:00, 286.39batch/s, mse=0.00073] \n",
      "Epoch 399: 100%|██████████| 46/46 [00:00<00:00, 273.83batch/s, mse=0.00073] \n",
      "Epoch 400: 100%|██████████| 46/46 [00:00<00:00, 307.30batch/s, mse=0.00073] \n",
      "Epoch 401: 100%|██████████| 46/46 [00:00<00:00, 286.62batch/s, mse=0.00073] \n",
      "Epoch 402: 100%|██████████| 46/46 [00:00<00:00, 372.31batch/s, mse=0.00073] \n",
      "Epoch 403: 100%|██████████| 46/46 [00:00<00:00, 371.78batch/s, mse=0.000729]\n",
      "Epoch 404: 100%|██████████| 46/46 [00:00<00:00, 324.56batch/s, mse=0.000729]\n",
      "Epoch 405: 100%|██████████| 46/46 [00:00<00:00, 373.51batch/s, mse=0.000729]\n",
      "Epoch 406: 100%|██████████| 46/46 [00:00<00:00, 335.80batch/s, mse=0.000729]\n",
      "Epoch 407: 100%|██████████| 46/46 [00:00<00:00, 437.56batch/s, mse=0.000729]\n",
      "Epoch 408: 100%|██████████| 46/46 [00:00<00:00, 341.62batch/s, mse=0.000728]\n",
      "Epoch 409: 100%|██████████| 46/46 [00:00<00:00, 394.38batch/s, mse=0.000728]\n",
      "Epoch 410: 100%|██████████| 46/46 [00:00<00:00, 333.67batch/s, mse=0.000728]\n",
      "Epoch 411: 100%|██████████| 46/46 [00:00<00:00, 389.65batch/s, mse=0.000728]\n",
      "Epoch 412: 100%|██████████| 46/46 [00:00<00:00, 346.56batch/s, mse=0.000727]\n",
      "Epoch 413: 100%|██████████| 46/46 [00:00<00:00, 380.89batch/s, mse=0.000727]\n",
      "Epoch 414: 100%|██████████| 46/46 [00:00<00:00, 362.93batch/s, mse=0.000727]\n",
      "Epoch 415: 100%|██████████| 46/46 [00:00<00:00, 380.91batch/s, mse=0.000727]\n",
      "Epoch 416: 100%|██████████| 46/46 [00:00<00:00, 322.40batch/s, mse=0.000727]\n",
      "Epoch 417: 100%|██████████| 46/46 [00:00<00:00, 382.72batch/s, mse=0.000726]\n",
      "Epoch 418: 100%|██████████| 46/46 [00:00<00:00, 400.30batch/s, mse=0.000726]\n",
      "Epoch 419: 100%|██████████| 46/46 [00:00<00:00, 375.27batch/s, mse=0.000726]\n",
      "Epoch 420: 100%|██████████| 46/46 [00:00<00:00, 502.57batch/s, mse=0.000726]\n",
      "Epoch 421: 100%|██████████| 46/46 [00:00<00:00, 361.57batch/s, mse=0.000726]\n",
      "Epoch 422: 100%|██████████| 46/46 [00:00<00:00, 475.50batch/s, mse=0.000725]\n",
      "Epoch 423: 100%|██████████| 46/46 [00:00<00:00, 476.53batch/s, mse=0.000725]\n",
      "Epoch 424: 100%|██████████| 46/46 [00:00<00:00, 391.89batch/s, mse=0.000725]\n",
      "Epoch 425: 100%|██████████| 46/46 [00:00<00:00, 416.34batch/s, mse=0.000725]\n",
      "Epoch 426: 100%|██████████| 46/46 [00:00<00:00, 450.79batch/s, mse=0.000725]\n",
      "Epoch 427: 100%|██████████| 46/46 [00:00<00:00, 383.16batch/s, mse=0.000724]\n",
      "Epoch 428: 100%|██████████| 46/46 [00:00<00:00, 534.81batch/s, mse=0.000724]\n",
      "Epoch 429: 100%|██████████| 46/46 [00:00<00:00, 322.41batch/s, mse=0.000724]\n",
      "Epoch 430: 100%|██████████| 46/46 [00:00<00:00, 424.22batch/s, mse=0.000724]\n",
      "Epoch 431: 100%|██████████| 46/46 [00:00<00:00, 427.71batch/s, mse=0.000724]\n",
      "Epoch 432: 100%|██████████| 46/46 [00:00<00:00, 394.72batch/s, mse=0.000724]\n",
      "Epoch 433: 100%|██████████| 46/46 [00:00<00:00, 433.03batch/s, mse=0.000723]\n",
      "Epoch 434: 100%|██████████| 46/46 [00:00<00:00, 462.11batch/s, mse=0.000723]\n",
      "Epoch 435: 100%|██████████| 46/46 [00:00<00:00, 427.51batch/s, mse=0.000723]\n",
      "Epoch 436: 100%|██████████| 46/46 [00:00<00:00, 364.44batch/s, mse=0.000723]\n",
      "Epoch 437: 100%|██████████| 46/46 [00:00<00:00, 496.26batch/s, mse=0.000723]\n",
      "Epoch 438: 100%|██████████| 46/46 [00:00<00:00, 372.87batch/s, mse=0.000722]\n",
      "Epoch 439: 100%|██████████| 46/46 [00:00<00:00, 497.13batch/s, mse=0.000722]\n",
      "Epoch 440: 100%|██████████| 46/46 [00:00<00:00, 374.83batch/s, mse=0.000722]\n",
      "Epoch 441: 100%|██████████| 46/46 [00:00<00:00, 497.06batch/s, mse=0.000722]\n",
      "Epoch 442: 100%|██████████| 46/46 [00:00<00:00, 359.61batch/s, mse=0.000722]\n",
      "Epoch 443: 100%|██████████| 46/46 [00:00<00:00, 386.04batch/s, mse=0.000721]\n",
      "Epoch 444: 100%|██████████| 46/46 [00:00<00:00, 396.08batch/s, mse=0.000721]\n",
      "Epoch 445: 100%|██████████| 46/46 [00:00<00:00, 348.34batch/s, mse=0.000721]\n",
      "Epoch 446: 100%|██████████| 46/46 [00:00<00:00, 437.50batch/s, mse=0.000721]\n",
      "Epoch 447: 100%|██████████| 46/46 [00:00<00:00, 374.29batch/s, mse=0.000721]\n",
      "Epoch 448: 100%|██████████| 46/46 [00:00<00:00, 516.99batch/s, mse=0.00072] \n",
      "Epoch 449: 100%|██████████| 46/46 [00:00<00:00, 402.53batch/s, mse=0.00072] \n",
      "Epoch 450: 100%|██████████| 46/46 [00:00<00:00, 496.93batch/s, mse=0.00072] \n",
      "Epoch 451: 100%|██████████| 46/46 [00:00<00:00, 415.53batch/s, mse=0.00072] \n",
      "Epoch 452: 100%|██████████| 46/46 [00:00<00:00, 419.26batch/s, mse=0.00072] \n",
      "Epoch 453: 100%|██████████| 46/46 [00:00<00:00, 476.52batch/s, mse=0.00072] \n",
      "Epoch 454: 100%|██████████| 46/46 [00:00<00:00, 385.22batch/s, mse=0.000719]\n",
      "Epoch 455: 100%|██████████| 46/46 [00:00<00:00, 522.52batch/s, mse=0.000719]\n",
      "Epoch 456: 100%|██████████| 46/46 [00:00<00:00, 388.09batch/s, mse=0.000719]\n",
      "Epoch 457: 100%|██████████| 46/46 [00:00<00:00, 414.69batch/s, mse=0.000719]\n",
      "Epoch 458: 100%|██████████| 46/46 [00:00<00:00, 475.98batch/s, mse=0.000719]\n",
      "Epoch 459: 100%|██████████| 46/46 [00:00<00:00, 405.01batch/s, mse=0.000718]\n",
      "Epoch 460: 100%|██████████| 46/46 [00:00<00:00, 476.04batch/s, mse=0.000718]\n",
      "Epoch 461: 100%|██████████| 46/46 [00:00<00:00, 404.14batch/s, mse=0.000718]\n",
      "Epoch 462: 100%|██████████| 46/46 [00:00<00:00, 486.12batch/s, mse=0.000718]\n",
      "Epoch 463: 100%|██████████| 46/46 [00:00<00:00, 380.73batch/s, mse=0.000718]\n",
      "Epoch 464: 100%|██████████| 46/46 [00:00<00:00, 466.79batch/s, mse=0.000718]\n",
      "Epoch 465: 100%|██████████| 46/46 [00:00<00:00, 377.62batch/s, mse=0.000717]\n",
      "Epoch 466: 100%|██████████| 46/46 [00:00<00:00, 458.66batch/s, mse=0.000717]\n",
      "Epoch 467: 100%|██████████| 46/46 [00:00<00:00, 342.16batch/s, mse=0.000717]\n",
      "Epoch 468: 100%|██████████| 46/46 [00:00<00:00, 380.83batch/s, mse=0.000717]\n",
      "Epoch 469: 100%|██████████| 46/46 [00:00<00:00, 528.70batch/s, mse=0.000717]\n",
      "Epoch 470: 100%|██████████| 46/46 [00:00<00:00, 390.03batch/s, mse=0.000716]\n",
      "Epoch 471: 100%|██████████| 46/46 [00:00<00:00, 403.58batch/s, mse=0.000716]\n",
      "Epoch 472: 100%|██████████| 46/46 [00:00<00:00, 442.13batch/s, mse=0.000716]\n",
      "Epoch 473: 100%|██████████| 46/46 [00:00<00:00, 378.59batch/s, mse=0.000716]\n",
      "Epoch 474: 100%|██████████| 46/46 [00:00<00:00, 542.35batch/s, mse=0.000716]\n",
      "Epoch 475: 100%|██████████| 46/46 [00:00<00:00, 367.02batch/s, mse=0.000716]\n",
      "Epoch 476: 100%|██████████| 46/46 [00:00<00:00, 511.76batch/s, mse=0.000715]\n",
      "Epoch 477: 100%|██████████| 46/46 [00:00<00:00, 375.81batch/s, mse=0.000715]\n",
      "Epoch 478: 100%|██████████| 46/46 [00:00<00:00, 374.11batch/s, mse=0.000715]\n",
      "Epoch 479: 100%|██████████| 46/46 [00:00<00:00, 517.03batch/s, mse=0.000715]\n",
      "Epoch 480: 100%|██████████| 46/46 [00:00<00:00, 338.69batch/s, mse=0.000715]\n",
      "Epoch 481: 100%|██████████| 46/46 [00:00<00:00, 463.42batch/s, mse=0.000715]\n",
      "Epoch 482: 100%|██████████| 46/46 [00:00<00:00, 341.36batch/s, mse=0.000714]\n",
      "Epoch 483: 100%|██████████| 46/46 [00:00<00:00, 455.40batch/s, mse=0.000714]\n",
      "Epoch 484: 100%|██████████| 46/46 [00:00<00:00, 343.97batch/s, mse=0.000714]\n",
      "Epoch 485: 100%|██████████| 46/46 [00:00<00:00, 438.34batch/s, mse=0.000714]\n",
      "Epoch 486: 100%|██████████| 46/46 [00:00<00:00, 367.74batch/s, mse=0.000714]\n",
      "Epoch 487: 100%|██████████| 46/46 [00:00<00:00, 401.22batch/s, mse=0.000714]\n",
      "Epoch 488: 100%|██████████| 46/46 [00:00<00:00, 390.17batch/s, mse=0.000713]\n",
      "Epoch 489: 100%|██████████| 46/46 [00:00<00:00, 342.05batch/s, mse=0.000713]\n",
      "Epoch 490: 100%|██████████| 46/46 [00:00<00:00, 416.12batch/s, mse=0.000713]\n",
      "Epoch 491: 100%|██████████| 46/46 [00:00<00:00, 361.50batch/s, mse=0.000713]\n",
      "Epoch 492: 100%|██████████| 46/46 [00:00<00:00, 441.29batch/s, mse=0.000713]\n",
      "Epoch 493: 100%|██████████| 46/46 [00:00<00:00, 372.52batch/s, mse=0.000713]\n",
      "Epoch 494: 100%|██████████| 46/46 [00:00<00:00, 512.46batch/s, mse=0.000712]\n",
      "Epoch 495: 100%|██████████| 46/46 [00:00<00:00, 423.86batch/s, mse=0.000712]\n",
      "Epoch 496: 100%|██████████| 46/46 [00:00<00:00, 472.61batch/s, mse=0.000712]\n",
      "Epoch 497: 100%|██████████| 46/46 [00:00<00:00, 402.25batch/s, mse=0.000712]\n",
      "Epoch 498: 100%|██████████| 46/46 [00:00<00:00, 479.78batch/s, mse=0.000712]\n",
      "Epoch 499: 100%|██████████| 46/46 [00:00<00:00, 373.56batch/s, mse=0.000712]\n"
     ]
    }
   ],
   "source": [
    "nn_train(X_train = X_train, y_train= y_train, model=model, loss_fn = loss_fn, optimizer_name = optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, log=False, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033063103910535574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open('models/NN/59_NN.pkl', 'rb'))\n",
    "y_test_pred = model(X_test)\n",
    "test_loss = loss_fn(y_test_pred, y_test)\n",
    "test_loss = float(test_loss)\n",
    "test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
