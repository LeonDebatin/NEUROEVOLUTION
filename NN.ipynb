{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "seed=0\n",
    "import random\n",
    "random.seed(seed)\n",
    "import torch\n",
    "import pickle\n",
    "import \n",
    "torch.manual_seed(seed)\n",
    "X_train = pd.read_csv('datamart/X_train_clipped_scaled.csv').values\n",
    "y_train = pd.read_csv('datamart/y_lactose_train.csv').values\n",
    "X_test = pd.read_csv('datamart/X_test_clipped_scaled.csv').values\n",
    "y_test = pd.read_csv('datamart/y_lactose_test.csv').values\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "      layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cross_validation(X_train, y_train, model, loss, optimizer_name, lr=None, weight_decay=0.0, n_epochs=1000, batch_size=10,  kf=None, log=False, id=None ):\n",
    "\n",
    "    if log:\n",
    "        name = 'logs/NN/' + f'{id}' + '_random_search.csv'\n",
    "        with open(name, 'w', newline='\\n') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=';')\n",
    "            w.writerow(['id']+['fold']+['best_score']+['best_generations'])\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        X_train_cross, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_train_cross, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        model.apply(reset_weights) \n",
    "        # Initialize the optimizer, we have to do this each fold otherwise the optimizer will continue from where it left off\n",
    "        #this is also why wi initialize it within the cross validaion \n",
    "        optimizer = get_optimizer(model.parameters(),optimizer_name, lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        batch_start = torch.arange(0, len(X_train_cross), batch_size)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                bar.set_description(f\"Epoch {epoch}\")\n",
    "                for start in bar:\n",
    "                    # Take a batch\n",
    "                    X_batch = X_train_cross[start:start+batch_size]\n",
    "                    y_batch = y_train_cross[start:start+batch_size]\n",
    "                    # Forward pass\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    # Backward pass\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update weights\n",
    "                    \n",
    "                    # Print progress\n",
    "                    bar.set_postfix(mse=float(loss))\n",
    "            \n",
    "            # Evaluate accuracy at end of each epoch\n",
    "            model.eval()\n",
    "            y_pred = model(X_val)\n",
    "            mse = loss_fn(y_pred, y_val)\n",
    "            mse = float(mse)    \n",
    "            if log:\n",
    "                name = 'logs/NN/' + f'{id}'+ '_random_search.csv'\n",
    "                with open(name, 'a', newline='\\n') as csvfile:\n",
    "                    w = csv.writer(csvfile, delimiter=';')\n",
    "                    w.writerow([id]+[fold]+[mse]+[epoch])\n",
    "                    \n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "                \n",
    "        results.append(mse)\n",
    "        \n",
    "    avg_mse = np.mean(results)\n",
    "    print(avg_mse)\n",
    "    return avg_mse\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(12, 1),\n",
    "        nn.Linear(1, 1),\n",
    "\n",
    "    )\n",
    "\n",
    "def eval_model(model, X_test, y_test):\n",
    "    pred = model.forward(X_test)[:, 0]\n",
    "    return mean_squared_error(y_test, pred)\n",
    "\n",
    "def create_random_model():\n",
    "    n_neurons_1 = random.randint(1,10)\n",
    "    n_neurons_2 = random.randint(1,10)\n",
    "    r1 = random.uniform(0,1)\n",
    "    r2 = random.uniform(0,1)\n",
    "    model = nn.Sequential(nn.Linear( 12, n_neurons_1))\n",
    "    \n",
    "    if r1 < 0.25:\n",
    "        model.add_module('relu1', nn.ReLU())\n",
    "        \n",
    "    elif 0.25 < r1< 0.5:\n",
    "        model.add_module('sigmoid1', nn.Sigmoid())\n",
    "    \n",
    "    if 0.5 < r1 < 0.75:\n",
    "        model.add_module('linear', nn.Linear(n_neurons_1, n_neurons_2))\n",
    "        \n",
    "        if r2 < 0.25:\n",
    "            model.add_module('relu2', nn.ReLU())\n",
    "        elif 0.25 < r2 < 0.5:\n",
    "            model.add_module('sigmoid2', nn.Sigmoid())\n",
    "        model.add_module('output', nn.Linear(n_neurons_2, 1))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    model.add_module('output', nn.Linear(n_neurons_1,1))\n",
    "    # if r2 > 0.25:\n",
    "    #     model.add_module('relu2', nn.ReLU())\n",
    "    # elif 0.25 < r2 < 0.5:\n",
    "    #     model.add_module('sigmoid2', nn.Sigmoid())\n",
    "    return model\n",
    "\n",
    "def get_optimizer(model_params, optimizer_name, lr, weight_decay=0.0):\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return optim.SGD(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'ASGD':\n",
    "        return optim.ASGD(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        return optim.RMSprop(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer name')\n",
    "\n",
    "def random_optimizer():\n",
    "    r = random.uniform(0,1)\n",
    "    if r < 0.2:\n",
    "        return 'Adam'\n",
    "    elif 0.2 < r < 0.4:\n",
    "        return 'SGD'\n",
    "    elif 0.4 < r < 0.6:\n",
    "        return 'Adadelta'\n",
    "    elif 0.6 < r < 0.8:\n",
    "        return 'ASGD'\n",
    "    return 'RMSprop'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(X_train, y_train, model, loss_fn, optimizer_name, lr=None, weight_decay=0.0, n_epochs=1000, batch_size=10, log=False, id=None):\n",
    "    if log:\n",
    "        name = 'logs/NN/' + f'{id}' + '_training_log.csv'\n",
    "        with open(name, 'w', newline='\\n') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=';')\n",
    "            w.writerow(['id', 'epoch', 'train_loss'])\n",
    "    \n",
    "    # Reset model weights\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    \n",
    "    optimizer = get_optimizer(model.parameters(), optimizer_name, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # Take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Store the batch loss\n",
    "                train_losses.append(loss.item())\n",
    "                \n",
    "\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        if log:\n",
    "            with open(name, 'a', newline='\\n') as csvfile:\n",
    "                w = csv.writer(csvfile, delimiter=';')\n",
    "                w.writerow([id, epoch, avg_train_loss])\n",
    "        \n",
    "        # Save best model\n",
    "        \n",
    "    with open('models/NN/'+str(id)+'_NN.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003593212505802512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003593212505802512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.001\n",
    "n_epochs = 400   # number of epochs to run\n",
    "batch_size = 10   # size of each batch\n",
    "optimizer_name = 'Adam'\n",
    "# Early stopping parameters\n",
    "patience = 50  # how many epochs to wait after last improvement\n",
    "weight_decay = 0.01  # L2 regularization\n",
    "# K-Fold Cross Validation\n",
    "kf = RepeatedKFold(n_splits=10, random_state=seed, n_repeats=2)\n",
    "fold_results = []\n",
    "nn_cross_validation(X_train=X_train, y_train=y_train, model=model, loss=loss_fn, optimizer_name=optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, kf=kf, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027184876427054406\n",
      "Iteration 0\n",
      "New best validation score: 0.027184876427054406\n",
      "New best test score: 0.045174188911914825\n",
      "0.0035887762438505887\n",
      "Iteration 1\n",
      "New best validation score: 0.0035887762438505887\n",
      "New best test score: 0.003364142496138811\n",
      "0.0037052291445434093\n",
      "0.3592709541320801\n",
      "1.0420036554336547\n",
      "0.0037788374815136195\n",
      "0.004837098531424999\n",
      "0.011469313222914935\n",
      "0.003877949435263872\n",
      "24.292771530151366\n",
      "0.005067973537370563\n",
      "10.860096454620361\n",
      "0.007869753520935774\n",
      "0.04726052777841687\n",
      "0.013748956006020307\n",
      "21.433744049072267\n",
      "23.871287155151368\n",
      "0.004341402556747198\n",
      "0.003556282725185156\n",
      "Iteration 18\n",
      "New best validation score: 0.003556282725185156\n",
      "New best test score: 0.0032749304082244635\n",
      "4.81184949874878\n",
      "21.46697998046875\n",
      "24.65494613647461\n",
      "0.004473681095987558\n",
      "0.038438836298882964\n",
      "0.00645664413459599\n",
      "23.747204399108888\n",
      "18.578235244750978\n",
      "0.005674108862876892\n",
      "0.0168074119836092\n",
      "0.9272774219512939\n",
      "0.009297988843172789\n",
      "19.947751235961913\n",
      "11.543501091003417\n",
      "0.04058038555085659\n",
      "0.785224461555481\n",
      "0.005321200471371412\n",
      "0.006812463141977787\n",
      "0.0036761822644621133\n",
      "0.0035546854604035618\n",
      "Iteration 38\n",
      "New best validation score: 0.0035546854604035618\n",
      "New best test score: 0.01694745570421219\n",
      "24.780571365356444\n",
      "23.27303123474121\n",
      "21.93710060119629\n",
      "17.07822380065918\n",
      "0.034914100915193556\n",
      "0.020143133029341696\n",
      "23.372715377807616\n",
      "0.02359674107283354\n",
      "23.762693023681642\n",
      "7.721284866333008\n",
      "0.015889680199325084\n",
      "0.004637722251936793\n",
      "24.048320007324218\n",
      "0.045570933073759076\n",
      "10.034813690185548\n",
      "23.88106803894043\n",
      "0.010729491431266069\n",
      "0.0039015353191643952\n",
      "21.034117889404296\n",
      "0.004516552062705159\n",
      "0.0035344767849892376\n",
      "Iteration 59\n",
      "New best validation score: 0.0035344767849892376\n",
      "New best test score: 0.0033063103910535574\n",
      "0.09871916305273772\n",
      "2.6706486701965333\n",
      "0.00629582917317748\n",
      "0.05592681765556336\n",
      "19.872317504882812\n",
      "0.0033583077136427166\n",
      "Iteration 65\n",
      "New best validation score: 0.0033583077136427166\n",
      "New best test score: 0.003528827801346779\n",
      "0.003954723430797458\n",
      "0.641548615694046\n",
      "8.117727184295655\n",
      "0.008500665612518787\n",
      "0.006539286393672228\n",
      "20.66175765991211\n",
      "0.009720000810921193\n",
      "0.04944970160722732\n",
      "0.010135954292491079\n",
      "0.00460477564483881\n",
      "0.05429800786077976\n",
      "0.009617180097848177\n",
      "0.011270369868725538\n",
      "16.89431838989258\n",
      "0.0046229114755988125\n",
      "8.758682918548583\n",
      "0.0036926754750311376\n",
      "0.004408581322059036\n",
      "18.796975708007814\n",
      "0.009957154281437397\n",
      "0.004218442365527153\n",
      "23.557658004760743\n",
      "0.017228632513433696\n",
      "18.78204689025879\n",
      "5.388642358779907\n",
      "0.4826348528265953\n",
      "0.10989826172590256\n",
      "0.11202552132308483\n",
      "0.0190424345433712\n",
      "11.645800399780274\n",
      "0.004060674831271172\n",
      "0.008281121216714381\n",
      "22.907355117797852\n",
      "0.04439294934272766\n",
      "0.003672437183558941\n",
      "0.004138111369684339\n",
      "19.892618942260743\n",
      "0.007048160070553422\n",
      "0.006630795542150736\n",
      "0.008550019934773445\n",
      "8.688708591461182\n",
      "0.019043741561472417\n",
      "0.157463239133358\n",
      "0.005283089447766542\n",
      "0.0034636616706848144\n",
      "0.003705941466614604\n",
      "2.489814782142639\n",
      "1.3423814177513123\n",
      "24.385652160644533\n",
      "nan\n",
      "25.760528564453125\n",
      "0.003523253556340933\n",
      "0.006684942450374365\n",
      "0.0038093391340225935\n",
      "19.64596824645996\n",
      "0.029583821631968023\n",
      "0.036404092237353324\n",
      "0.016903734765946865\n",
      "0.05554969264194369\n",
      "12.641384696960449\n",
      "25.499383544921876\n",
      "0.004400040907785297\n",
      "0.00940361968241632\n",
      "0.09048309028148652\n",
      "0.008049257379025222\n",
      "0.014496078062802554\n",
      "0.013876750506460667\n",
      "0.0042741026263684034\n",
      "0.003534443490207195\n",
      "0.008432300388813018\n",
      "4.407124102115631\n",
      "0.0035675080958753823\n",
      "nan\n",
      "0.004891301784664392\n",
      "0.0991789609193802\n",
      "6.6852106094360355\n",
      "26.479711532592773\n",
      "0.05440979273989797\n",
      "0.0036816894076764583\n",
      "19.770086669921874\n",
      "0.003541626920923591\n",
      "21.75205307006836\n",
      "15.592680931091309\n",
      "23.131001281738282\n",
      "0.007346171792596579\n",
      "0.004103477438911795\n",
      "0.003968317713588476\n",
      "0.010561871761456131\n",
      "0.042672837525606154\n",
      "0.006883795652538538\n",
      "0.0196302792057395\n",
      "0.007517880387604236\n",
      "0.0337920643389225\n",
      "0.01187715157866478\n",
      "15.396112632751464\n",
      "4.125724291801452\n",
      "0.009267638158053159\n",
      "24.001233291625976\n",
      "0.01160421445965767\n",
      "0.0059065714478492735\n",
      "0.009773638565093279\n",
      "0.01301562786102295\n",
      "24.231087493896485\n",
      "0.0036763472016900778\n",
      "0.008722412213683129\n",
      "2.1393373012542725\n",
      "0.006180489342659712\n",
      "0.06403111293911934\n",
      "0.0037475054617971183\n",
      "0.004685296583920718\n",
      "0.0052434253506362435\n",
      "0.010016370937228203\n",
      "0.031215360946953298\n",
      "2.1626279294490813\n",
      "1.2183815240859985\n",
      "0.004033259907737374\n",
      "0.003678628010675311\n",
      "24.267693710327148\n",
      "0.03060334324836731\n",
      "0.004911656910553575\n",
      "0.2759789556264877\n",
      "0.004148093517869711\n",
      "0.009656118415296077\n",
      "0.014626405388116836\n",
      "0.0063495197333395485\n",
      "0.0036136887967586516\n",
      "0.0035710742231458426\n",
      "0.05247182585299015\n",
      "0.003955878596752882\n",
      "0.01568588027730584\n",
      "19.232813453674318\n",
      "0.36616689041256906\n",
      "9.743378067016602\n",
      "12.581956768035889\n",
      "0.004330104123800993\n",
      "10.460018157958984\n",
      "24.682525634765625\n",
      "0.006596256513148546\n",
      "0.004454712802544236\n",
      "0.015359994769096375\n",
      "0.009126022923737764\n",
      "12.694480228424073\n",
      "1.111191688477993\n",
      "0.030605407245457173\n",
      "0.015700760949403047\n",
      "0.005223376071080565\n",
      "0.06493921484798193\n",
      "0.014383381232619286\n",
      "0.03543983269482851\n",
      "0.004279071278870106\n",
      "3.5884578227996826\n",
      "0.21836646390147507\n",
      "26.43009834289551\n",
      "0.01673606541007757\n",
      "0.016618772689253093\n",
      "0.006330884713679552\n",
      "0.03566151969134808\n",
      "0.012682795897126198\n",
      "0.005993816535919905\n",
      "0.0035471841227263214\n",
      "0.005037284735590219\n",
      "0.013444425724446773\n",
      "22.827303314208983\n",
      "0.04634609781205654\n",
      "0.012196110747754573\n",
      "0.003542032930999994\n",
      "20.694883346557617\n",
      "0.03138024592772126\n",
      "0.010272026527673006\n",
      "0.004239176213741302\n",
      "0.007593514770269394\n",
      "0.008584376284852624\n",
      "21.14426460266113\n",
      "0.004120349930599332\n",
      "0.0042523124255239965\n",
      "0.29612727016210555\n",
      "0.018976907432079314\n",
      "0.00705618616193533\n",
      "11.882295417785645\n",
      "0.012469298020005227\n",
      "22.273357772827147\n",
      "0.008694108482450247\n",
      "0.003626467287540436\n",
      "0.03545901626348495\n",
      "0.003724943706765771\n",
      "0.021076111402362586\n",
      "16.105654335021974\n",
      "0.6279972026124596\n",
      "0.00584912970662117\n",
      "0.00355624589137733\n",
      "22.638121795654296\n",
      "0.0047542529180645944\n",
      "5.820284461975097\n",
      "0.0037721217144280673\n",
      "22.61165294647217\n",
      "24.425165939331055\n",
      "0.005933496356010437\n",
      "12.444156646728516\n",
      "7.163054752349853\n",
      "0.020873583294451235\n",
      "23.322143936157225\n",
      "0.003632196644321084\n",
      "0.03576516546308994\n",
      "0.00472268471494317\n",
      "1.8325272457208484\n",
      "0.003436244884505868\n",
      "0.0037042387761175633\n",
      "0.0036188978236168625\n",
      "0.3506959736347198\n",
      "17.114547348022462\n",
      "0.3429337128996849\n",
      "23.50716323852539\n",
      "9.887889099121093\n",
      "0.006993874348700047\n",
      "15.918976974487304\n",
      "0.013866640161722898\n",
      "21.301023483276367\n",
      "20.31520195007324\n",
      "0.0035917725879698994\n",
      "0.0036059300880879165\n",
      "0.005823127459734678\n",
      "21.59839782714844\n",
      "0.3557818029075861\n",
      "0.003676358191296458\n",
      "6.841853046417237\n",
      "0.004187823180109263\n",
      "0.016137947142124177\n",
      "0.006949585909023881\n",
      "0.006145260017365217\n",
      "0.011167938448488712\n",
      "0.007259277859702706\n",
      "0.0035608378704637287\n",
      "28.066990280151366\n",
      "0.027717671543359756\n",
      "0.003756934544071555\n",
      "25.78028793334961\n",
      "0.011042759474366903\n",
      "0.008797842916101218\n",
      "0.040377387404441835\n",
      "0.006804494839161635\n",
      "0.003641209378838539\n",
      "22.366984558105468\n",
      "0.5775298222899437\n",
      "11.010526275634765\n",
      "0.01496994998306036\n",
      "0.003879585117101669\n",
      "0.006214812304824591\n",
      "0.015563408471643924\n",
      "0.09582056477665901\n",
      "0.04833608157932758\n",
      "0.005050851404666901\n",
      "0.003738323785364628\n",
      "0.00562360342592001\n",
      "0.0035682918038219213\n",
      "0.004915431095287204\n",
      "0.005031790304929018\n",
      "6.689454980939627\n",
      "17.904665565490724\n",
      "24.041818618774414\n",
      "17.369893264770507\n",
      "0.0053204722236841915\n",
      "0.14908220581710338\n",
      "0.02787328530102968\n",
      "22.50695343017578\n",
      "0.003731229528784752\n",
      "0.0035882383584976197\n",
      "0.00577737633138895\n",
      "23.659321594238282\n",
      "0.004236041055992246\n",
      "24.576008987426757\n",
      "0.013877125643193721\n",
      "0.005509806191548705\n",
      "0.038672896847128865\n",
      "25.199922180175783\n",
      "0.008562217000871897\n",
      "27.49808578491211\n",
      "0.06950727328658104\n",
      "0.031712033040821555\n",
      "0.007571981474757195\n",
      "22.073005294799806\n",
      "0.005686621647328138\n",
      "0.004483314324170351\n",
      "21.568526458740234\n",
      "0.0035319484304636717\n",
      "0.0038086190819740296\n",
      "0.003760977927595377\n",
      "0.0036422024946659803\n",
      "0.0035476754419505596\n",
      "0.013027870934456587\n",
      "0.19879129976034166\n",
      "23.106549072265626\n",
      "0.008952740859240294\n",
      "0.02395556475967169\n",
      "20.229336166381835\n",
      "2.3540235996246337\n",
      "6.1745452880859375\n",
      "0.005455370247364044\n",
      "24.10491065979004\n",
      "0.013953237142413855\n",
      "17.238679504394533\n",
      "0.005791214387863874\n",
      "0.008502114657312631\n",
      "23.61732063293457\n",
      "0.009836572874337434\n",
      "0.004457154357805848\n",
      "0.0035819466225802898\n",
      "0.003539231698960066\n",
      "23.190593338012697\n",
      "10.703715896606445\n",
      "23.961817359924318\n",
      "21.50683937072754\n",
      "0.008561011170968413\n",
      "0.009166804049164057\n",
      "9.625553512573243\n",
      "12.863403940200806\n",
      "0.0043109802063554525\n",
      "0.024563392624258995\n",
      "0.005530686816200614\n",
      "0.00399379157461226\n",
      "0.023215199820697308\n",
      "18.889366912841798\n",
      "0.49718737602233887\n",
      "1.12954580783844\n",
      "20.49896068572998\n",
      "0.20623497888445855\n",
      "0.01626419508829713\n",
      "0.015126081276684999\n",
      "0.004068867489695549\n",
      "0.0041085629723966125\n",
      "0.00421534045599401\n",
      "23.72761764526367\n",
      "0.12177700996398926\n",
      "3.367188775539398\n",
      "20.50663833618164\n",
      "0.10157250314950943\n",
      "0.0036845368798822165\n",
      "0.003667381312698126\n",
      "0.10757264122366905\n",
      "0.023408733727410435\n",
      "0.005493535799905658\n",
      "0.0045476826839148995\n",
      "8.712784385681152\n",
      "0.04039666820317507\n",
      "21.784619522094726\n",
      "17.40223503112793\n",
      "0.011035299953073263\n",
      "22.201247024536134\n",
      "0.025859412085264922\n",
      "8.191480445861817\n",
      "0.10603210926055909\n",
      "0.003941555926576257\n",
      "10.104322147369384\n",
      "0.07121708495542407\n",
      "19.40888557434082\n",
      "0.004239633539691567\n",
      "13.081088638305664\n",
      "0.04350776001811028\n",
      "0.014238380827009678\n",
      "0.0042901495937258\n",
      "0.003945639356970787\n",
      "0.016962736193090678\n",
      "18.06111125946045\n",
      "0.009892406966537237\n",
      "0.0035887901205569504\n",
      "0.006128929043188691\n",
      "0.011658591777086258\n",
      "0.04070748807862401\n",
      "2.354962412128225\n",
      "0.05454706400632858\n",
      "0.003798202984035015\n",
      "0.003920783195644617\n",
      "0.13115003779530526\n",
      "0.10929517447948456\n",
      "26.011311721801757\n",
      "1.2251296401023866\n",
      "0.004957366595044732\n",
      "10.180703735351562\n",
      "0.036950438469648364\n",
      "17.55970401763916\n",
      "23.884918212890625\n",
      "0.0036732850596308706\n",
      "0.03406464792788029\n",
      "0.07744795316830277\n",
      "0.008193421084433794\n",
      "0.011937535740435124\n",
      "0.011416513845324517\n",
      "0.02095124190673232\n",
      "0.006384130846709013\n",
      "0.0041365519165992735\n",
      "0.1384976664558053\n",
      "0.0037133521400392056\n",
      "0.6189558836631477\n",
      "0.023992336913943292\n",
      "0.006595380837097764\n",
      "0.0040409809444099665\n",
      "15.915060424804688\n",
      "0.01025350084528327\n",
      "24.569264602661132\n",
      "0.013315575383603572\n",
      "0.0050091038458049296\n",
      "23.14444465637207\n",
      "19.91997833251953\n",
      "10.011399841308593\n",
      "1.0137935876846313\n",
      "0.0035775826778262853\n",
      "0.004230245342478156\n",
      "0.004136888589709997\n",
      "15.86204719543457\n",
      "1.6138051006942988\n",
      "0.00617136862128973\n",
      "0.009113642945885658\n",
      "0.003543091984465718\n",
      "0.003621953446418047\n",
      "2.5680599212646484\n",
      "0.5620029836893081\n",
      "0.003935200069099665\n",
      "0.08501288937404752\n",
      "25.402856826782227\n",
      "0.0036083102226257325\n",
      "22.924248695373535\n",
      "0.006350081553682685\n",
      "0.021830601524561642\n",
      "12.803029632568359\n",
      "0.004458083305507898\n",
      "19.785931396484376\n",
      "0.3620837092399597\n",
      "0.008600515965372323\n",
      "0.008535535261034965\n",
      "0.04812619984149933\n",
      "0.010152786411345005\n",
      "11.504486179351806\n",
      "0.01764069739729166\n",
      "nan\n",
      "0.01293882541358471\n",
      "0.0037795967888087033\n",
      "14.105042266845704\n",
      "7.049874782562256\n",
      "0.004176098061725498\n",
      "0.0037032270804047583\n",
      "0.005409474717453122\n",
      "0.07232673913240432\n",
      "22.163838195800782\n",
      "0.004145853826776147\n",
      "24.29098434448242\n",
      "0.008415673766285181\n",
      "11.231965398788452\n",
      "26.238779830932618\n",
      "0.003529200004413724\n",
      "0.005119466269388795\n",
      "0.0038893436547368767\n",
      "0.004252062458544969\n",
      "0.006856789346784353\n",
      "8.149221801757813\n",
      "0.003944544773548842\n",
      "0.04559993781149387\n",
      "0.004855012614279985\n",
      "12.414806175231934\n",
      "0.08981790244579316\n",
      "0.027258956665173174\n",
      "0.14466844797134398\n",
      "12.935760498046875\n",
      "0.0035748334135860204\n",
      "0.014474221039563417\n",
      "0.006606586975976825\n",
      "0.003546201903373003\n",
      "0.006339295534417033\n",
      "0.0131713991984725\n",
      "0.06910755075514316\n",
      "0.009148641396313906\n",
      "0.008492694329470396\n",
      "23.265038299560548\n",
      "0.43472184501588346\n",
      "0.004418080067262053\n",
      "0.016051902156323194\n",
      "0.21406460404396058\n",
      "20.91169090270996\n",
      "0.0038250554352998734\n",
      "0.0038671079091727734\n",
      "0.3551409050822258\n",
      "0.008637184090912343\n",
      "23.747953414916992\n",
      "5.888523244857788\n",
      "0.027565540745854378\n",
      "0.0036827769596129655\n",
      "0.0035679570864886045\n",
      "0.0037576740141958\n",
      "0.041974852234125136\n",
      "23.65871238708496\n",
      "0.007745753508061171\n",
      "0.004171572392806411\n",
      "18.688084220886232\n",
      "19.64982795715332\n",
      "0.003501810319721699\n",
      "0.012728020828217269\n",
      "0.0037369170691818\n",
      "0.19296472184360028\n",
      "0.007252314873039722\n",
      "0.004410219378769398\n",
      "0.11599580645561218\n",
      "23.464808654785156\n",
      "0.004319864651188254\n",
      "0.007472462952136993\n",
      "21.319376373291014\n",
      "0.29155482752248646\n",
      "0.003679396351799369\n",
      "22.4665225982666\n",
      "18.449020385742188\n",
      "0.0074202586896717545\n",
      "0.021825279295444488\n",
      "0.003634298499673605\n",
      "nan\n",
      "0.009600646328181028\n",
      "0.009492485038936137\n",
      "24.140377044677734\n",
      "7.450854396820068\n",
      "0.27118399888277056\n",
      "24.912126541137695\n",
      "24.14319953918457\n",
      "19.475030517578126\n",
      "0.004378779139369726\n",
      "0.01527081374078989\n",
      "0.027184705063700677\n",
      "0.010558148659765721\n",
      "0.012726458627730608\n",
      "0.10302703157067299\n",
      "0.09465445727109909\n",
      "0.05262447921559214\n",
      "0.10746025145053864\n",
      "0.007474056258797645\n",
      "0.004053011955693364\n",
      "23.397877502441407\n",
      "24.58652973175049\n",
      "24.511151123046876\n",
      "0.17929905131459237\n",
      "22.937391662597655\n",
      "0.0035826968960464\n",
      "0.015545562654733659\n",
      "5.441114044189453\n",
      "0.015348860248923301\n",
      "20.98300094604492\n",
      "19.834292602539062\n",
      "0.008746978919953107\n",
      "12.971699905395507\n",
      "18.899102020263673\n",
      "23.234358978271484\n",
      "0.00812318604439497\n",
      "0.062494618073105815\n",
      "4.287756037712097\n",
      "19.573371124267577\n",
      "0.01265323143452406\n",
      "18.98812141418457\n",
      "18.57676429748535\n",
      "0.47290639514103533\n",
      "0.003971318388357759\n",
      "17.9749267578125\n",
      "4.8568359375\n",
      "0.007086050324141979\n",
      "0.09364646263420581\n",
      "0.004152152547612786\n",
      "0.013615294173359872\n",
      "0.01904014740139246\n",
      "0.006089714309200645\n",
      "0.006122278980910778\n",
      "11.79885654449463\n",
      "22.11574058532715\n",
      "0.009721092134714126\n",
      "0.3244830921292305\n",
      "0.00469293836504221\n",
      "0.041190104372799394\n",
      "16.46914348602295\n",
      "12.202384948730469\n",
      "0.0104721050709486\n",
      "23.12068328857422\n",
      "0.09795975983142853\n",
      "0.004108160082250833\n",
      "0.0063233501743525265\n",
      "0.07293545491993428\n",
      "0.015837142802774908\n",
      "0.005584650905802846\n",
      "10.921949195861817\n",
      "0.004151424253359437\n",
      "21.20258483886719\n",
      "18.4221435546875\n",
      "21.385519409179686\n",
      "0.006820482295006513\n",
      "0.010944546852260827\n",
      "19.82527141571045\n",
      "0.18061286248266697\n",
      "0.12371530905365943\n",
      "0.038996937498450276\n",
      "23.22374382019043\n",
      "0.22548661306500434\n",
      "5.172121942043304\n",
      "0.21699431017041207\n",
      "23.00980567932129\n",
      "0.45833594277501105\n",
      "0.003538913559168577\n",
      "0.004014253430068493\n",
      "0.018971646204590797\n",
      "24.895577239990235\n",
      "11.605440521240235\n",
      "0.003798018442466855\n",
      "0.003834670642390847\n",
      "0.00472344383597374\n",
      "27.55228958129883\n",
      "0.10407033115625382\n",
      "0.013960688468068838\n",
      "2.1083502173423767\n",
      "0.1527088288217783\n",
      "0.059943081066012385\n",
      "0.003721130546182394\n",
      "0.07563275322318078\n",
      "0.052203729003667834\n",
      "19.174728393554688\n",
      "17.42767467498779\n",
      "25.438059616088868\n",
      "16.437994956970215\n",
      "0.00587109443731606\n",
      "0.003670936357229948\n",
      "5.583851170539856\n",
      "0.024429507553577423\n",
      "26.46062316894531\n",
      "0.004014171008020639\n",
      "24.10698471069336\n",
      "0.006583200627937913\n",
      "0.07608781158924102\n",
      "0.0037570744287222626\n",
      "0.02887409464456141\n",
      "11.192772483825683\n",
      "12.960403060913086\n",
      "0.006246036384254694\n",
      "4.160657298564911\n",
      "0.008002822939306497\n",
      "0.004036679258570075\n",
      "9.813928031921387\n",
      "0.010185022000223398\n",
      "2.714312291145325\n",
      "0.003600728744640946\n",
      "0.0036325227934867143\n",
      "0.007984419260174036\n",
      "0.0038517055101692675\n",
      "24.843209075927735\n",
      "13.97271671295166\n",
      "0.004783056490123272\n",
      "0.0036552282981574535\n",
      "0.015267913229763508\n",
      "22.301709365844726\n",
      "0.003975437907502055\n",
      "22.26052131652832\n",
      "0.0036450011655688288\n",
      "0.007129628770053386\n",
      "25.05243835449219\n",
      "0.09664203273132443\n",
      "0.004638529429212212\n",
      "0.00574941961094737\n",
      "0.004380822088569403\n",
      "11.818921279907226\n",
      "1.9352129220962524\n",
      "0.00353210661560297\n",
      "0.011303926631808282\n",
      "0.0037403931375592945\n",
      "0.5986637301743031\n",
      "0.0043329746462404724\n",
      "0.00864363918080926\n",
      "0.00849581863731146\n",
      "0.004500774294137954\n",
      "14.33518009185791\n",
      "17.169161224365233\n",
      "0.6944863796234131\n",
      "0.0056324111297726635\n",
      "0.043825264740735295\n",
      "28.56448097229004\n",
      "0.003521872218698263\n",
      "0.013749902043491602\n",
      "0.11088011800311506\n",
      "0.033229536190629\n",
      "0.6856959164142609\n",
      "0.004904387658461929\n",
      "24.85262451171875\n",
      "0.004410294117406011\n",
      "0.0035777232144027947\n",
      "1.1147491060197354\n",
      "6.90201244354248\n",
      "0.019838372711092234\n",
      "0.011340912664309144\n",
      "25.96426773071289\n",
      "9.434005737304688\n",
      "0.0070750052109360695\n",
      "0.4552451953291893\n",
      "0.017588392877951265\n",
      "12.806316375732422\n",
      "18.778240966796876\n",
      "0.012311427108943462\n",
      "0.043827064149081704\n",
      "0.10022753402590752\n",
      "0.0038344174157828093\n",
      "0.003720828564837575\n",
      "25.510102844238283\n",
      "16.804744529724122\n",
      "10.923665809631348\n",
      "18.523257064819337\n",
      "0.01255675107240677\n",
      "0.005098503921180964\n",
      "0.0039533648639917375\n",
      "0.005086655355989933\n",
      "0.003606557846069336\n",
      "6.96916618347168\n",
      "0.004130602767691016\n",
      "0.0036605958826839923\n",
      "0.0036122644785791637\n",
      "0.016871693637222053\n",
      "5.06769437789917\n",
      "0.04246470890939236\n",
      "0.21178706921637058\n",
      "1.4331439018249512\n",
      "0.003641989780589938\n",
      "0.004410598147660494\n",
      "0.04689660333096981\n",
      "0.011914638336747885\n",
      "0.010974082723259925\n",
      "23.797657775878907\n",
      "15.467708778381347\n",
      "0.005796054936945438\n",
      "11.502883148193359\n",
      "15.163232040405273\n",
      "0.030588372237980367\n",
      "24.776610946655275\n",
      "11.316823387145996\n",
      "23.393561935424806\n",
      "0.005931871896609664\n",
      "0.018353318516165017\n",
      "0.00971198957413435\n",
      "0.010585731267929077\n",
      "21.065857315063475\n",
      "22.488917541503906\n",
      "24.236865997314453\n",
      "0.1642823450267315\n",
      "0.012978543527424335\n",
      "26.91250648498535\n",
      "0.005297153955325484\n",
      "0.026442924235016107\n",
      "0.003423584857955575\n",
      "0.01261533540673554\n",
      "21.688165283203126\n",
      "15.031231307983399\n",
      "0.025386616587638855\n",
      "0.03729347661137581\n",
      "22.996114349365236\n",
      "24.156953430175783\n",
      "0.0036378408316522838\n",
      "22.147311782836915\n",
      "24.073475646972657\n",
      "0.014374066982418299\n",
      "20.223234939575196\n",
      "22.70073699951172\n",
      "0.010606992058455944\n",
      "0.06063038557767868\n",
      "0.015874383971095085\n",
      "0.003580347588285804\n",
      "0.003576148860156536\n",
      "1.0090459704399108\n",
      "0.0037229088135063647\n",
      "0.03629389787092805\n",
      "0.016706834360957146\n",
      "4.0824000358581545\n",
      "0.016378775984048844\n",
      "0.003825279790908098\n",
      "10.693396663665771\n",
      "0.017616858892142772\n",
      "0.003761432133615017\n",
      "0.007011275924742222\n",
      "21.938158798217774\n",
      "0.007370831351727248\n",
      "10.745143127441406\n",
      "0.010575110046193003\n",
      "0.011625872738659383\n",
      "14.50422248840332\n",
      "0.004334638640284538\n",
      "0.0047880413476377726\n",
      "0.00853380635380745\n",
      "0.004704261478036642\n",
      "21.853163528442384\n",
      "0.0036539706867188214\n",
      "0.011344333831220865\n",
      "0.004392260499298572\n",
      "0.005706919077783823\n",
      "0.045254790037870404\n",
      "0.008036881871521472\n",
      "0.22282039672136306\n",
      "0.010731305740773679\n",
      "0.13879482671618462\n",
      "0.008142940886318683\n",
      "0.05772197172045708\n",
      "5.686120891571045\n",
      "0.023641026951372622\n",
      "0.005961814802139998\n",
      "0.004221984045580029\n",
      "0.041672483086586\n",
      "24.357428741455077\n",
      "0.00396964936517179\n",
      "23.28107109069824\n",
      "24.671731567382814\n",
      "0.03987255785614252\n",
      "0.0039572475478053095\n",
      "0.004085470736026764\n",
      "0.027442324534058572\n",
      "0.1442458838224411\n",
      "0.007090584561228752\n",
      "0.0036796601954847572\n",
      "12.64442253112793\n",
      "0.0064449581317603585\n",
      "0.004361691744998097\n",
      "10.456199836730956\n",
      "0.0073835855349898335\n",
      "0.014602272398769855\n",
      "22.89803237915039\n",
      "0.09174945428967476\n",
      "0.003601147048175335\n",
      "0.007557329256087542\n",
      "0.0037905648816376923\n",
      "0.00896156788803637\n",
      "15.101601219177246\n",
      "0.013977330178022385\n",
      "21.53122730255127\n",
      "24.56560516357422\n",
      "0.010634729731827974\n",
      "0.004555999394506216\n",
      "21.83758201599121\n",
      "23.528679275512694\n",
      "0.010991083085536956\n",
      "16.750600051879882\n",
      "0.0038257899694144728\n",
      "23.066573333740234\n",
      "4.293394279479981\n",
      "0.006646210420876741\n",
      "0.0036868553142994642\n",
      "0.013967476971447467\n",
      "0.006816082494333386\n",
      "0.044129544496536256\n",
      "0.009092525392770768\n",
      "0.03727652654051781\n",
      "0.0082939513027668\n",
      "2.385771059989929\n",
      "0.03882809709757566\n",
      "0.017232158873230218\n",
      "26.35772285461426\n",
      "0.004190424224361777\n",
      "0.5011267930269241\n",
      "0.00465001673437655\n",
      "0.548673215508461\n",
      "0.0037867236416786907\n",
      "0.007532842643558979\n",
      "23.21273307800293\n",
      "0.3308625012636185\n",
      "0.012068750616163016\n",
      "0.0035847723484039307\n",
      "24.67442207336426\n",
      "0.005236774776130914\n",
      "0.05291024912148714\n",
      "22.055960845947265\n",
      "10.490579605102539\n",
      "0.00587930572219193\n",
      "0.003515804186463356\n",
      "0.007503593619912863\n",
      "23.92104721069336\n",
      "0.005099421506747604\n",
      "20.158171272277833\n",
      "0.0037484795320779085\n",
      "0.00819460116326809\n",
      "0.005205319542437792\n",
      "10.094231843948364\n",
      "9.698743152618409\n",
      "0.004563836520537734\n",
      "0.005404001660645008\n",
      "0.027460227813571693\n",
      "9.243833637237548\n",
      "13.913385963439941\n",
      "8.615164852142334\n",
      "23.855558395385742\n",
      "0.004869892029091716\n",
      "0.14373261332511902\n",
      "0.0036701470613479613\n",
      "0.003705583466216922\n",
      "0.007603818643838167\n",
      "0.08143419548869132\n",
      "0.004369376739487052\n",
      "25.15842933654785\n",
      "23.97657012939453\n",
      "0.012925635371357203\n",
      "0.018121771793812514\n",
      "2.352931785583496\n",
      "20.966804885864256\n",
      "0.0036120898090302943\n",
      "0.013577181100845336\n",
      "10.592815589904784\n",
      "0.008612265624105931\n",
      "22.08254470825195\n",
      "23.292561721801757\n",
      "0.00821612924337387\n",
      "24.6282657623291\n",
      "24.172168350219728\n",
      "0.00358132878318429\n",
      "0.006776573648676276\n",
      "0.005426725558936596\n",
      "0.008779385220259428\n",
      "1.024746796488762\n",
      "26.008495330810547\n",
      "13.954641532897949\n",
      "0.038126854225993156\n",
      "0.00990526396781206\n",
      "0.009772635158151388\n",
      "0.1711897782050073\n",
      "15.267361831665038\n",
      "0.003661043103784323\n",
      "0.10419273469597101\n",
      "21.733768463134766\n",
      "0.005160711891949177\n",
      "0.00352605520747602\n",
      "0.004458193015307188\n",
      "0.01949299927800894\n",
      "23.171365356445314\n",
      "0.11971945315599442\n",
      "0.01041674236766994\n",
      "3.4539688348770143\n",
      "0.0036674288101494314\n",
      "21.884906768798828\n",
      "26.147506713867188\n",
      "0.0035665125586092474\n",
      "0.19146426171064376\n",
      "0.007768847234547138\n",
      "0.0132818385027349\n",
      "0.011536700278520584\n",
      "0.05867741536349058\n",
      "23.86341667175293\n"
     ]
    }
   ],
   "source": [
    "kf = RepeatedKFold(n_splits=10, random_state=seed, n_repeats=2)\n",
    "batch_sizes = [5,10,15,20,25]\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "n_epoches = [25, 50, 75, 100, 250, 500, 1000]\n",
    "weight_decays = [0.0, 0.1, 0.01, 0.001, 0.0001] \n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_score = np.inf\n",
    "\n",
    "name = 'logs/NN/' + 'random_search' + '.csv'\n",
    "with open(name, 'w', newline='\\n') as csvfile:\n",
    "    w = csv.writer(csvfile, delimiter=';')\n",
    "    w.writerow(['id']+['model']+['optimizer']+['learning_rate']+['l2']+['batch_size']+['n_epochs']+['cv_score'])\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    model = create_random_model()\n",
    "    lr = random.choice(learning_rates)\n",
    "    optimizer_name = random_optimizer()\n",
    "    n_epochs = random.choice(n_epoches)\n",
    "    weight_decay = random.choice(weight_decays)\n",
    "    score = nn_cross_validation(X_train=X_train, y_train=y_train, model=model, loss=loss_fn, optimizer_name = optimizer_name,lr=lr,  n_epochs=n_epochs, batch_size=batch_size,kf=kf, log=True, id=i)\n",
    "    \n",
    "    name = 'logs/NN/' + 'random_search' + '.csv'\n",
    "    with open(name, 'a', newline='\\n') as csvfile:\n",
    "        w = csv.writer(csvfile, delimiter=';')\n",
    "        w.writerow([i]+[f'{model}']+[f'{optimizer_name}']+[lr]+[weight_decay]+[batch_size]+[n_epochs]+[score])\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        print('Iteration '+ str(i))\n",
    "        print(f'New best validation score: {best_score}')\n",
    "        \n",
    "        nn_train(X_train = X_train, y_train= y_train, model=model, loss_fn = loss_fn, optimizer_name = optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, log=True, id=i)\n",
    "        model = pickle.load(open('models/NN/' + str(i) + '_NN.pkl', 'rb'))\n",
    "        y_test_pred = model(X_test)\n",
    "        test_score = loss_fn(y_test_pred, y_test)\n",
    "        test_score = float(test_score)\n",
    "        print(f'New best test score: {test_score}')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                              model optimizer  \\\n",
      "0    0  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "1    1  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "2    2  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "3    3  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "4    4  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "5    5  Sequential(\\n  (0): Linear(in_features=12, out...      ASGD   \n",
      "6    6  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "7    7  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "8    8  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "9    9  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "10  10  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "11  11  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "12  12  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "13  13  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "14  14  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "15  15  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "16  16  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "17  17  Sequential(\\n  (0): Linear(in_features=12, out...      Adam   \n",
      "18  18  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "19  19  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "20  20  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "21  21  Sequential(\\n  (0): Linear(in_features=12, out...  Adadelta   \n",
      "22  22  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "23  23  Sequential(\\n  (0): Linear(in_features=12, out...   RMSprop   \n",
      "24  24  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "\n",
      "    learning_rate      l2  batch_size  n_epochs   cv_score  \n",
      "0          0.1000  0.0100          20      1000   0.027185  \n",
      "1          0.0010  0.0000          20      1000   0.003589  \n",
      "2          0.0010  0.0000          25       500   0.003705  \n",
      "3          0.0010  0.0010          15       100   0.359271  \n",
      "4          0.0001  0.0010          25      1000   1.042004  \n",
      "5          0.0100  0.1000           5        25   0.003779  \n",
      "6          0.1000  0.0001          25        75   0.004837  \n",
      "7          0.0100  0.1000          20       250   0.011469  \n",
      "8          0.1000  0.0100          25        50   0.003878  \n",
      "9          0.0100  0.1000          10        25  24.292772  \n",
      "10         0.1000  0.0100          10       250   0.005068  \n",
      "11         0.1000  0.0010          25       100  10.860096  \n",
      "12         0.0100  0.0000          15        50   0.007870  \n",
      "13         0.1000  0.1000          15        25   0.047261  \n",
      "14         0.0001  0.1000          10       500   0.013749  \n",
      "15         0.0001  0.1000          25        25  21.433744  \n",
      "16         0.0001  0.0000          10       250  23.871287  \n",
      "17         0.1000  0.0010          15       250   0.004341  \n",
      "18         0.0100  0.0010           5       500   0.003556  \n",
      "19         0.0010  0.0001          25        75   4.811849  \n",
      "20         0.0001  0.0001          15       100  21.466980  \n",
      "21         0.0010  0.0010          25        25  24.654946  \n",
      "22         0.1000  0.0100           5        75   0.004474  \n",
      "23         0.1000  0.0000          25        25   0.038439  \n",
      "24         0.1000  0.0001          25       500   0.006457  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('logs/NN/random_search.csv', delimiter=';')\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[df['cv_score']<0.005, 'cv_score'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                              model optimizer  \\\n",
      "18  18  Sequential(\\n  (0): Linear(in_features=12, out...       SGD   \n",
      "\n",
      "    learning_rate     l2  batch_size  n_epochs  cv_score  \n",
      "18           0.01  0.001           5       500  0.003556  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.loc[df['cv_score'] == df['cv_score'].min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";\"Sequential(\n",
    "  (0): Linear(in_features=12, out_features=10, bias=True)\n",
    "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
    "  (sigmoid2): Sigmoid()\n",
    "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
    ")\";SGD;0.01;0.0001;5;500;0.0034930747002363204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear( 12, 10), nn.Linear(10, 2), nn.Sigmoid(), nn.Linear(2, 1))\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_name = 'SGD'\n",
    "lr = 0.01\n",
    "weight_decay = 0.0001\n",
    "n_epochs = 500\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset trainable parameters of layer = Linear(in_features=12, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=2, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=1, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 46/46 [00:00<00:00, 380.06batch/s, mse=0.863]\n",
      "Epoch 1: 100%|██████████| 46/46 [00:00<00:00, 440.38batch/s, mse=0.00797]\n",
      "Epoch 2: 100%|██████████| 46/46 [00:00<00:00, 428.07batch/s, mse=0.000146]\n",
      "Epoch 3: 100%|██████████| 46/46 [00:00<00:00, 396.14batch/s, mse=0.000186]\n",
      "Epoch 4: 100%|██████████| 46/46 [00:00<00:00, 603.13batch/s, mse=0.000381]\n",
      "Epoch 5: 100%|██████████| 46/46 [00:00<00:00, 377.77batch/s, mse=0.000569]\n",
      "Epoch 6: 100%|██████████| 46/46 [00:00<00:00, 380.06batch/s, mse=0.000719]\n",
      "Epoch 7: 100%|██████████| 46/46 [00:00<00:00, 471.42batch/s, mse=0.000833]\n",
      "Epoch 8: 100%|██████████| 46/46 [00:00<00:00, 403.85batch/s, mse=0.000918]\n",
      "Epoch 9: 100%|██████████| 46/46 [00:00<00:00, 425.68batch/s, mse=0.000981]\n",
      "Epoch 10: 100%|██████████| 46/46 [00:00<00:00, 527.66batch/s, mse=0.00103] \n",
      "Epoch 11: 100%|██████████| 46/46 [00:00<00:00, 354.47batch/s, mse=0.00106] \n",
      "Epoch 12: 100%|██████████| 46/46 [00:00<00:00, 537.85batch/s, mse=0.00109] \n",
      "Epoch 13: 100%|██████████| 46/46 [00:00<00:00, 521.57batch/s, mse=0.0011]  \n",
      "Epoch 14: 100%|██████████| 46/46 [00:00<00:00, 405.32batch/s, mse=0.00111] \n",
      "Epoch 15: 100%|██████████| 46/46 [00:00<00:00, 529.44batch/s, mse=0.00112] \n",
      "Epoch 16: 100%|██████████| 46/46 [00:00<00:00, 513.31batch/s, mse=0.00112] \n",
      "Epoch 17: 100%|██████████| 46/46 [00:00<00:00, 398.09batch/s, mse=0.00113] \n",
      "Epoch 18: 100%|██████████| 46/46 [00:00<00:00, 466.22batch/s, mse=0.00112] \n",
      "Epoch 19: 100%|██████████| 46/46 [00:00<00:00, 495.57batch/s, mse=0.00112] \n",
      "Epoch 20: 100%|██████████| 46/46 [00:00<00:00, 396.76batch/s, mse=0.00112] \n",
      "Epoch 21: 100%|██████████| 46/46 [00:00<00:00, 446.65batch/s, mse=0.00111] \n",
      "Epoch 22: 100%|██████████| 46/46 [00:00<00:00, 493.35batch/s, mse=0.0011]  \n",
      "Epoch 23: 100%|██████████| 46/46 [00:00<00:00, 405.14batch/s, mse=0.0011]  \n",
      "Epoch 24: 100%|██████████| 46/46 [00:00<00:00, 513.45batch/s, mse=0.00109] \n",
      "Epoch 25: 100%|██████████| 46/46 [00:00<00:00, 419.77batch/s, mse=0.00108] \n",
      "Epoch 26: 100%|██████████| 46/46 [00:00<00:00, 513.80batch/s, mse=0.00107] \n",
      "Epoch 27: 100%|██████████| 46/46 [00:00<00:00, 434.17batch/s, mse=0.00106] \n",
      "Epoch 28: 100%|██████████| 46/46 [00:00<00:00, 490.91batch/s, mse=0.00106] \n",
      "Epoch 29: 100%|██████████| 46/46 [00:00<00:00, 479.32batch/s, mse=0.00105] \n",
      "Epoch 30: 100%|██████████| 46/46 [00:00<00:00, 390.07batch/s, mse=0.00104] \n",
      "Epoch 31: 100%|██████████| 46/46 [00:00<00:00, 523.88batch/s, mse=0.00103] \n",
      "Epoch 32: 100%|██████████| 46/46 [00:00<00:00, 400.87batch/s, mse=0.00103] \n",
      "Epoch 33: 100%|██████████| 46/46 [00:00<00:00, 533.22batch/s, mse=0.00102] \n",
      "Epoch 34: 100%|██████████| 46/46 [00:00<00:00, 511.11batch/s, mse=0.00101] \n",
      "Epoch 35: 100%|██████████| 46/46 [00:00<00:00, 405.10batch/s, mse=0.00101] \n",
      "Epoch 36: 100%|██████████| 46/46 [00:00<00:00, 533.17batch/s, mse=0.001]   \n",
      "Epoch 37: 100%|██████████| 46/46 [00:00<00:00, 483.10batch/s, mse=0.000994]\n",
      "Epoch 38: 100%|██████████| 46/46 [00:00<00:00, 432.89batch/s, mse=0.000988]\n",
      "Epoch 39: 100%|██████████| 46/46 [00:00<00:00, 550.96batch/s, mse=0.000982]\n",
      "Epoch 40: 100%|██████████| 46/46 [00:00<00:00, 394.21batch/s, mse=0.000977]\n",
      "Epoch 41: 100%|██████████| 46/46 [00:00<00:00, 530.27batch/s, mse=0.000972]\n",
      "Epoch 42: 100%|██████████| 46/46 [00:00<00:00, 537.21batch/s, mse=0.000967]\n",
      "Epoch 43: 100%|██████████| 46/46 [00:00<00:00, 338.20batch/s, mse=0.000962]\n",
      "Epoch 44: 100%|██████████| 46/46 [00:00<00:00, 490.84batch/s, mse=0.000957]\n",
      "Epoch 45: 100%|██████████| 46/46 [00:00<00:00, 410.78batch/s, mse=0.000953]\n",
      "Epoch 46: 100%|██████████| 46/46 [00:00<00:00, 537.35batch/s, mse=0.000949]\n",
      "Epoch 47: 100%|██████████| 46/46 [00:00<00:00, 543.42batch/s, mse=0.000944]\n",
      "Epoch 48: 100%|██████████| 46/46 [00:00<00:00, 387.62batch/s, mse=0.00094] \n",
      "Epoch 49: 100%|██████████| 46/46 [00:00<00:00, 531.27batch/s, mse=0.000937]\n",
      "Epoch 50: 100%|██████████| 46/46 [00:00<00:00, 544.12batch/s, mse=0.000933]\n",
      "Epoch 51: 100%|██████████| 46/46 [00:00<00:00, 403.37batch/s, mse=0.000929]\n",
      "Epoch 52: 100%|██████████| 46/46 [00:00<00:00, 519.59batch/s, mse=0.000926]\n",
      "Epoch 53: 100%|██████████| 46/46 [00:00<00:00, 536.48batch/s, mse=0.000923]\n",
      "Epoch 54: 100%|██████████| 46/46 [00:00<00:00, 402.17batch/s, mse=0.00092] \n",
      "Epoch 55: 100%|██████████| 46/46 [00:00<00:00, 524.22batch/s, mse=0.000917]\n",
      "Epoch 56: 100%|██████████| 46/46 [00:00<00:00, 531.46batch/s, mse=0.000914]\n",
      "Epoch 57: 100%|██████████| 46/46 [00:00<00:00, 385.08batch/s, mse=0.000911]\n",
      "Epoch 58: 100%|██████████| 46/46 [00:00<00:00, 518.66batch/s, mse=0.000908]\n",
      "Epoch 59: 100%|██████████| 46/46 [00:00<00:00, 500.69batch/s, mse=0.000906]\n",
      "Epoch 60: 100%|██████████| 46/46 [00:00<00:00, 403.76batch/s, mse=0.000903]\n",
      "Epoch 61: 100%|██████████| 46/46 [00:00<00:00, 518.97batch/s, mse=0.000901]\n",
      "Epoch 62: 100%|██████████| 46/46 [00:00<00:00, 443.71batch/s, mse=0.000899]\n",
      "Epoch 63: 100%|██████████| 46/46 [00:00<00:00, 412.06batch/s, mse=0.000896]\n",
      "Epoch 64: 100%|██████████| 46/46 [00:00<00:00, 476.10batch/s, mse=0.000894]\n",
      "Epoch 65: 100%|██████████| 46/46 [00:00<00:00, 345.94batch/s, mse=0.000892]\n",
      "Epoch 66: 100%|██████████| 46/46 [00:00<00:00, 491.75batch/s, mse=0.00089] \n",
      "Epoch 67: 100%|██████████| 46/46 [00:00<00:00, 508.02batch/s, mse=0.000888]\n",
      "Epoch 68: 100%|██████████| 46/46 [00:00<00:00, 398.00batch/s, mse=0.000886]\n",
      "Epoch 69: 100%|██████████| 46/46 [00:00<00:00, 453.20batch/s, mse=0.000884]\n",
      "Epoch 70: 100%|██████████| 46/46 [00:00<00:00, 517.76batch/s, mse=0.000882]\n",
      "Epoch 71: 100%|██████████| 46/46 [00:00<00:00, 391.26batch/s, mse=0.000881]\n",
      "Epoch 72: 100%|██████████| 46/46 [00:00<00:00, 543.17batch/s, mse=0.000879]\n",
      "Epoch 73: 100%|██████████| 46/46 [00:00<00:00, 443.91batch/s, mse=0.000877]\n",
      "Epoch 74: 100%|██████████| 46/46 [00:00<00:00, 361.14batch/s, mse=0.000876]\n",
      "Epoch 75: 100%|██████████| 46/46 [00:00<00:00, 432.22batch/s, mse=0.000874]\n",
      "Epoch 76: 100%|██████████| 46/46 [00:00<00:00, 355.56batch/s, mse=0.000873]\n",
      "Epoch 77: 100%|██████████| 46/46 [00:00<00:00, 401.14batch/s, mse=0.000871]\n",
      "Epoch 78: 100%|██████████| 46/46 [00:00<00:00, 346.97batch/s, mse=0.000869]\n",
      "Epoch 79: 100%|██████████| 46/46 [00:00<00:00, 397.15batch/s, mse=0.000868]\n",
      "Epoch 80: 100%|██████████| 46/46 [00:00<00:00, 525.67batch/s, mse=0.000867]\n",
      "Epoch 81: 100%|██████████| 46/46 [00:00<00:00, 404.73batch/s, mse=0.000865]\n",
      "Epoch 82: 100%|██████████| 46/46 [00:00<00:00, 431.32batch/s, mse=0.000864]\n",
      "Epoch 83: 100%|██████████| 46/46 [00:00<00:00, 543.16batch/s, mse=0.000863]\n",
      "Epoch 84: 100%|██████████| 46/46 [00:00<00:00, 398.16batch/s, mse=0.000861]\n",
      "Epoch 85: 100%|██████████| 46/46 [00:00<00:00, 513.19batch/s, mse=0.00086] \n",
      "Epoch 86: 100%|██████████| 46/46 [00:00<00:00, 497.46batch/s, mse=0.000859]\n",
      "Epoch 87: 100%|██████████| 46/46 [00:00<00:00, 412.30batch/s, mse=0.000858]\n",
      "Epoch 88: 100%|██████████| 46/46 [00:00<00:00, 538.33batch/s, mse=0.000857]\n",
      "Epoch 89: 100%|██████████| 46/46 [00:00<00:00, 378.93batch/s, mse=0.000856]\n",
      "Epoch 90: 100%|██████████| 46/46 [00:00<00:00, 442.07batch/s, mse=0.000854]\n",
      "Epoch 91: 100%|██████████| 46/46 [00:00<00:00, 494.36batch/s, mse=0.000853]\n",
      "Epoch 92: 100%|██████████| 46/46 [00:00<00:00, 367.67batch/s, mse=0.000852]\n",
      "Epoch 93: 100%|██████████| 46/46 [00:00<00:00, 487.43batch/s, mse=0.000851]\n",
      "Epoch 94: 100%|██████████| 46/46 [00:00<00:00, 391.31batch/s, mse=0.00085] \n",
      "Epoch 95: 100%|██████████| 46/46 [00:00<00:00, 501.58batch/s, mse=0.000849]\n",
      "Epoch 96: 100%|██████████| 46/46 [00:00<00:00, 513.21batch/s, mse=0.000848]\n",
      "Epoch 97: 100%|██████████| 46/46 [00:00<00:00, 399.57batch/s, mse=0.000847]\n",
      "Epoch 98: 100%|██████████| 46/46 [00:00<00:00, 536.87batch/s, mse=0.000846]\n",
      "Epoch 99: 100%|██████████| 46/46 [00:00<00:00, 417.65batch/s, mse=0.000845]\n",
      "Epoch 100: 100%|██████████| 46/46 [00:00<00:00, 512.77batch/s, mse=0.000844]\n",
      "Epoch 101: 100%|██████████| 46/46 [00:00<00:00, 475.97batch/s, mse=0.000843]\n",
      "Epoch 102: 100%|██████████| 46/46 [00:00<00:00, 342.17batch/s, mse=0.000842]\n",
      "Epoch 103: 100%|██████████| 46/46 [00:00<00:00, 466.78batch/s, mse=0.000842]\n",
      "Epoch 104: 100%|██████████| 46/46 [00:00<00:00, 392.70batch/s, mse=0.000841]\n",
      "Epoch 105: 100%|██████████| 46/46 [00:00<00:00, 376.74batch/s, mse=0.00084] \n",
      "Epoch 106: 100%|██████████| 46/46 [00:00<00:00, 514.46batch/s, mse=0.000839]\n",
      "Epoch 107: 100%|██████████| 46/46 [00:00<00:00, 375.01batch/s, mse=0.000838]\n",
      "Epoch 108: 100%|██████████| 46/46 [00:00<00:00, 536.26batch/s, mse=0.000837]\n",
      "Epoch 109: 100%|██████████| 46/46 [00:00<00:00, 440.25batch/s, mse=0.000837]\n",
      "Epoch 110: 100%|██████████| 46/46 [00:00<00:00, 448.47batch/s, mse=0.000836]\n",
      "Epoch 111: 100%|██████████| 46/46 [00:00<00:00, 518.24batch/s, mse=0.000835]\n",
      "Epoch 112: 100%|██████████| 46/46 [00:00<00:00, 387.19batch/s, mse=0.000834]\n",
      "Epoch 113: 100%|██████████| 46/46 [00:00<00:00, 486.59batch/s, mse=0.000833]\n",
      "Epoch 114: 100%|██████████| 46/46 [00:00<00:00, 427.29batch/s, mse=0.000833]\n",
      "Epoch 115: 100%|██████████| 46/46 [00:00<00:00, 451.92batch/s, mse=0.000832]\n",
      "Epoch 116: 100%|██████████| 46/46 [00:00<00:00, 543.94batch/s, mse=0.000831]\n",
      "Epoch 117: 100%|██████████| 46/46 [00:00<00:00, 490.33batch/s, mse=0.00083] \n",
      "Epoch 118: 100%|██████████| 46/46 [00:00<00:00, 375.50batch/s, mse=0.00083] \n",
      "Epoch 119: 100%|██████████| 46/46 [00:00<00:00, 475.80batch/s, mse=0.000829]\n",
      "Epoch 120: 100%|██████████| 46/46 [00:00<00:00, 382.11batch/s, mse=0.000828]\n",
      "Epoch 121: 100%|██████████| 46/46 [00:00<00:00, 519.17batch/s, mse=0.000827]\n",
      "Epoch 122: 100%|██████████| 46/46 [00:00<00:00, 417.15batch/s, mse=0.000827]\n",
      "Epoch 123: 100%|██████████| 46/46 [00:00<00:00, 544.22batch/s, mse=0.000826]\n",
      "Epoch 124: 100%|██████████| 46/46 [00:00<00:00, 475.85batch/s, mse=0.000825]\n",
      "Epoch 125: 100%|██████████| 46/46 [00:00<00:00, 501.68batch/s, mse=0.000825]\n",
      "Epoch 126: 100%|██████████| 46/46 [00:00<00:00, 507.28batch/s, mse=0.000824]\n",
      "Epoch 127: 100%|██████████| 46/46 [00:00<00:00, 350.68batch/s, mse=0.000823]\n",
      "Epoch 128: 100%|██████████| 46/46 [00:00<00:00, 476.58batch/s, mse=0.000823]\n",
      "Epoch 129: 100%|██████████| 46/46 [00:00<00:00, 380.14batch/s, mse=0.000822]\n",
      "Epoch 130: 100%|██████████| 46/46 [00:00<00:00, 489.11batch/s, mse=0.000821]\n",
      "Epoch 131: 100%|██████████| 46/46 [00:00<00:00, 408.23batch/s, mse=0.000821]\n",
      "Epoch 132: 100%|██████████| 46/46 [00:00<00:00, 550.38batch/s, mse=0.00082] \n",
      "Epoch 133: 100%|██████████| 46/46 [00:00<00:00, 466.95batch/s, mse=0.00082] \n",
      "Epoch 134: 100%|██████████| 46/46 [00:00<00:00, 472.54batch/s, mse=0.000819]\n",
      "Epoch 135: 100%|██████████| 46/46 [00:00<00:00, 553.62batch/s, mse=0.000818]\n",
      "Epoch 136: 100%|██████████| 46/46 [00:00<00:00, 399.29batch/s, mse=0.000818]\n",
      "Epoch 137: 100%|██████████| 46/46 [00:00<00:00, 505.83batch/s, mse=0.000817]\n",
      "Epoch 138: 100%|██████████| 46/46 [00:00<00:00, 419.58batch/s, mse=0.000817]\n",
      "Epoch 139: 100%|██████████| 46/46 [00:00<00:00, 540.62batch/s, mse=0.000816]\n",
      "Epoch 140: 100%|██████████| 46/46 [00:00<00:00, 522.90batch/s, mse=0.000815]\n",
      "Epoch 141: 100%|██████████| 46/46 [00:00<00:00, 502.37batch/s, mse=0.000815]\n",
      "Epoch 142: 100%|██████████| 46/46 [00:00<00:00, 508.17batch/s, mse=0.000814]\n",
      "Epoch 143: 100%|██████████| 46/46 [00:00<00:00, 390.36batch/s, mse=0.000814]\n",
      "Epoch 144: 100%|██████████| 46/46 [00:00<00:00, 568.13batch/s, mse=0.000813]\n",
      "Epoch 145: 100%|██████████| 46/46 [00:00<00:00, 427.07batch/s, mse=0.000813]\n",
      "Epoch 146: 100%|██████████| 46/46 [00:00<00:00, 416.01batch/s, mse=0.000812]\n",
      "Epoch 147: 100%|██████████| 46/46 [00:00<00:00, 535.91batch/s, mse=0.000811]\n",
      "Epoch 148: 100%|██████████| 46/46 [00:00<00:00, 372.75batch/s, mse=0.000811]\n",
      "Epoch 149: 100%|██████████| 46/46 [00:00<00:00, 504.55batch/s, mse=0.00081] \n",
      "Epoch 150: 100%|██████████| 46/46 [00:00<00:00, 375.33batch/s, mse=0.00081] \n",
      "Epoch 151: 100%|██████████| 46/46 [00:00<00:00, 518.87batch/s, mse=0.000809]\n",
      "Epoch 152: 100%|██████████| 46/46 [00:00<00:00, 405.01batch/s, mse=0.000809]\n",
      "Epoch 153: 100%|██████████| 46/46 [00:00<00:00, 549.08batch/s, mse=0.000808]\n",
      "Epoch 154: 100%|██████████| 46/46 [00:00<00:00, 465.33batch/s, mse=0.000808]\n",
      "Epoch 155: 100%|██████████| 46/46 [00:00<00:00, 452.09batch/s, mse=0.000807]\n",
      "Epoch 156: 100%|██████████| 46/46 [00:00<00:00, 550.04batch/s, mse=0.000807]\n",
      "Epoch 157: 100%|██████████| 46/46 [00:00<00:00, 382.84batch/s, mse=0.000806]\n",
      "Epoch 158: 100%|██████████| 46/46 [00:00<00:00, 525.70batch/s, mse=0.000806]\n",
      "Epoch 159: 100%|██████████| 46/46 [00:00<00:00, 461.32batch/s, mse=0.000805]\n",
      "Epoch 160: 100%|██████████| 46/46 [00:00<00:00, 514.78batch/s, mse=0.000805]\n",
      "Epoch 161: 100%|██████████| 46/46 [00:00<00:00, 563.59batch/s, mse=0.000804]\n",
      "Epoch 162: 100%|██████████| 46/46 [00:00<00:00, 391.39batch/s, mse=0.000804]\n",
      "Epoch 163: 100%|██████████| 46/46 [00:00<00:00, 501.71batch/s, mse=0.000803]\n",
      "Epoch 164: 100%|██████████| 46/46 [00:00<00:00, 408.35batch/s, mse=0.000803]\n",
      "Epoch 165: 100%|██████████| 46/46 [00:00<00:00, 567.08batch/s, mse=0.000802]\n",
      "Epoch 166: 100%|██████████| 46/46 [00:00<00:00, 408.92batch/s, mse=0.000802]\n",
      "Epoch 167: 100%|██████████| 46/46 [00:00<00:00, 550.01batch/s, mse=0.000801]\n",
      "Epoch 168: 100%|██████████| 46/46 [00:00<00:00, 419.37batch/s, mse=0.000801]\n",
      "Epoch 169: 100%|██████████| 46/46 [00:00<00:00, 550.13batch/s, mse=0.0008]  \n",
      "Epoch 170: 100%|██████████| 46/46 [00:00<00:00, 514.10batch/s, mse=0.0008]  \n",
      "Epoch 171: 100%|██████████| 46/46 [00:00<00:00, 375.37batch/s, mse=0.000799]\n",
      "Epoch 172: 100%|██████████| 46/46 [00:00<00:00, 550.45batch/s, mse=0.000799]\n",
      "Epoch 173: 100%|██████████| 46/46 [00:00<00:00, 404.61batch/s, mse=0.000798]\n",
      "Epoch 174: 100%|██████████| 46/46 [00:00<00:00, 576.85batch/s, mse=0.000798]\n",
      "Epoch 175: 100%|██████████| 46/46 [00:00<00:00, 407.83batch/s, mse=0.000798]\n",
      "Epoch 176: 100%|██████████| 46/46 [00:00<00:00, 556.15batch/s, mse=0.000797]\n",
      "Epoch 177: 100%|██████████| 46/46 [00:00<00:00, 456.63batch/s, mse=0.000797]\n",
      "Epoch 178: 100%|██████████| 46/46 [00:00<00:00, 486.07batch/s, mse=0.000796]\n",
      "Epoch 179: 100%|██████████| 46/46 [00:00<00:00, 578.72batch/s, mse=0.000796]\n",
      "Epoch 180: 100%|██████████| 46/46 [00:00<00:00, 383.26batch/s, mse=0.000795]\n",
      "Epoch 181: 100%|██████████| 46/46 [00:00<00:00, 461.94batch/s, mse=0.000795]\n",
      "Epoch 182: 100%|██████████| 46/46 [00:00<00:00, 462.16batch/s, mse=0.000794]\n",
      "Epoch 183: 100%|██████████| 46/46 [00:00<00:00, 506.08batch/s, mse=0.000794]\n",
      "Epoch 184: 100%|██████████| 46/46 [00:00<00:00, 421.09batch/s, mse=0.000794]\n",
      "Epoch 185: 100%|██████████| 46/46 [00:00<00:00, 556.26batch/s, mse=0.000793]\n",
      "Epoch 186: 100%|██████████| 46/46 [00:00<00:00, 399.72batch/s, mse=0.000793]\n",
      "Epoch 187: 100%|██████████| 46/46 [00:00<00:00, 556.28batch/s, mse=0.000792]\n",
      "Epoch 188: 100%|██████████| 46/46 [00:00<00:00, 407.24batch/s, mse=0.000792]\n",
      "Epoch 189: 100%|██████████| 46/46 [00:00<00:00, 585.39batch/s, mse=0.000791]\n",
      "Epoch 190: 100%|██████████| 46/46 [00:00<00:00, 423.17batch/s, mse=0.000791]\n",
      "Epoch 191: 100%|██████████| 46/46 [00:00<00:00, 547.89batch/s, mse=0.000791]\n",
      "Epoch 192: 100%|██████████| 46/46 [00:00<00:00, 412.64batch/s, mse=0.00079] \n",
      "Epoch 193: 100%|██████████| 46/46 [00:00<00:00, 549.98batch/s, mse=0.00079] \n",
      "Epoch 194: 100%|██████████| 46/46 [00:00<00:00, 423.43batch/s, mse=0.000789]\n",
      "Epoch 195: 100%|██████████| 46/46 [00:00<00:00, 548.81batch/s, mse=0.000789]\n",
      "Epoch 196: 100%|██████████| 46/46 [00:00<00:00, 384.79batch/s, mse=0.000789]\n",
      "Epoch 197: 100%|██████████| 46/46 [00:00<00:00, 485.71batch/s, mse=0.000788]\n",
      "Epoch 198: 100%|██████████| 46/46 [00:00<00:00, 452.76batch/s, mse=0.000788]\n",
      "Epoch 199: 100%|██████████| 46/46 [00:00<00:00, 501.53batch/s, mse=0.000787]\n",
      "Epoch 200: 100%|██████████| 46/46 [00:00<00:00, 556.47batch/s, mse=0.000787]\n",
      "Epoch 201: 100%|██████████| 46/46 [00:00<00:00, 419.63batch/s, mse=0.000787]\n",
      "Epoch 202: 100%|██████████| 46/46 [00:00<00:00, 556.37batch/s, mse=0.000786]\n",
      "Epoch 203: 100%|██████████| 46/46 [00:00<00:00, 407.71batch/s, mse=0.000786]\n",
      "Epoch 204: 100%|██████████| 46/46 [00:00<00:00, 569.84batch/s, mse=0.000785]\n",
      "Epoch 205: 100%|██████████| 46/46 [00:00<00:00, 401.21batch/s, mse=0.000785]\n",
      "Epoch 206: 100%|██████████| 46/46 [00:00<00:00, 538.92batch/s, mse=0.000785]\n",
      "Epoch 207: 100%|██████████| 46/46 [00:00<00:00, 356.66batch/s, mse=0.000784]\n",
      "Epoch 208: 100%|██████████| 46/46 [00:00<00:00, 454.98batch/s, mse=0.000784]\n",
      "Epoch 209: 100%|██████████| 46/46 [00:00<00:00, 389.22batch/s, mse=0.000783]\n",
      "Epoch 210: 100%|██████████| 46/46 [00:00<00:00, 536.21batch/s, mse=0.000783]\n",
      "Epoch 211: 100%|██████████| 46/46 [00:00<00:00, 369.10batch/s, mse=0.000783]\n",
      "Epoch 212: 100%|██████████| 46/46 [00:00<00:00, 563.53batch/s, mse=0.000782]\n",
      "Epoch 213: 100%|██████████| 46/46 [00:00<00:00, 363.23batch/s, mse=0.000782]\n",
      "Epoch 214: 100%|██████████| 46/46 [00:00<00:00, 373.07batch/s, mse=0.000782]\n",
      "Epoch 215: 100%|██████████| 46/46 [00:00<00:00, 386.85batch/s, mse=0.000781]\n",
      "Epoch 216: 100%|██████████| 46/46 [00:00<00:00, 336.08batch/s, mse=0.000781]\n",
      "Epoch 217: 100%|██████████| 46/46 [00:00<00:00, 397.08batch/s, mse=0.00078] \n",
      "Epoch 218: 100%|██████████| 46/46 [00:00<00:00, 368.46batch/s, mse=0.00078] \n",
      "Epoch 219: 100%|██████████| 46/46 [00:00<00:00, 419.20batch/s, mse=0.00078]\n",
      "Epoch 220: 100%|██████████| 46/46 [00:00<00:00, 382.22batch/s, mse=0.000779]\n",
      "Epoch 221: 100%|██████████| 46/46 [00:00<00:00, 461.47batch/s, mse=0.000779]\n",
      "Epoch 222: 100%|██████████| 46/46 [00:00<00:00, 419.57batch/s, mse=0.000779]\n",
      "Epoch 223: 100%|██████████| 46/46 [00:00<00:00, 457.26batch/s, mse=0.000778]\n",
      "Epoch 224: 100%|██████████| 46/46 [00:00<00:00, 427.82batch/s, mse=0.000778]\n",
      "Epoch 225: 100%|██████████| 46/46 [00:00<00:00, 354.38batch/s, mse=0.000777]\n",
      "Epoch 226: 100%|██████████| 46/46 [00:00<00:00, 439.63batch/s, mse=0.000777]\n",
      "Epoch 227: 100%|██████████| 46/46 [00:00<00:00, 448.37batch/s, mse=0.000777]\n",
      "Epoch 228: 100%|██████████| 46/46 [00:00<00:00, 473.03batch/s, mse=0.000776]\n",
      "Epoch 229: 100%|██████████| 46/46 [00:00<00:00, 411.14batch/s, mse=0.000776]\n",
      "Epoch 230: 100%|██████████| 46/46 [00:00<00:00, 573.86batch/s, mse=0.000776]\n",
      "Epoch 231: 100%|██████████| 46/46 [00:00<00:00, 388.16batch/s, mse=0.000775]\n",
      "Epoch 232: 100%|██████████| 46/46 [00:00<00:00, 495.09batch/s, mse=0.000775]\n",
      "Epoch 233: 100%|██████████| 46/46 [00:00<00:00, 401.05batch/s, mse=0.000775]\n",
      "Epoch 234: 100%|██████████| 46/46 [00:00<00:00, 526.74batch/s, mse=0.000774]\n",
      "Epoch 235: 100%|██████████| 46/46 [00:00<00:00, 394.02batch/s, mse=0.000774]\n",
      "Epoch 236: 100%|██████████| 46/46 [00:00<00:00, 504.89batch/s, mse=0.000774]\n",
      "Epoch 237: 100%|██████████| 46/46 [00:00<00:00, 381.93batch/s, mse=0.000773]\n",
      "Epoch 238: 100%|██████████| 46/46 [00:00<00:00, 531.31batch/s, mse=0.000773]\n",
      "Epoch 239: 100%|██████████| 46/46 [00:00<00:00, 367.40batch/s, mse=0.000773]\n",
      "Epoch 240: 100%|██████████| 46/46 [00:00<00:00, 504.91batch/s, mse=0.000772]\n",
      "Epoch 241: 100%|██████████| 46/46 [00:00<00:00, 375.61batch/s, mse=0.000772]\n",
      "Epoch 242: 100%|██████████| 46/46 [00:00<00:00, 481.37batch/s, mse=0.000772]\n",
      "Epoch 243: 100%|██████████| 46/46 [00:00<00:00, 339.15batch/s, mse=0.000771]\n",
      "Epoch 244: 100%|██████████| 46/46 [00:00<00:00, 502.76batch/s, mse=0.000771]\n",
      "Epoch 245: 100%|██████████| 46/46 [00:00<00:00, 389.94batch/s, mse=0.000771]\n",
      "Epoch 246: 100%|██████████| 46/46 [00:00<00:00, 496.62batch/s, mse=0.00077] \n",
      "Epoch 247: 100%|██████████| 46/46 [00:00<00:00, 386.53batch/s, mse=0.00077] \n",
      "Epoch 248: 100%|██████████| 46/46 [00:00<00:00, 496.15batch/s, mse=0.00077] \n",
      "Epoch 249: 100%|██████████| 46/46 [00:00<00:00, 388.82batch/s, mse=0.000769]\n",
      "Epoch 250: 100%|██████████| 46/46 [00:00<00:00, 486.48batch/s, mse=0.000769]\n",
      "Epoch 251: 100%|██████████| 46/46 [00:00<00:00, 361.79batch/s, mse=0.000769]\n",
      "Epoch 252: 100%|██████████| 46/46 [00:00<00:00, 512.85batch/s, mse=0.000768]\n",
      "Epoch 253: 100%|██████████| 46/46 [00:00<00:00, 371.63batch/s, mse=0.000768]\n",
      "Epoch 254: 100%|██████████| 46/46 [00:00<00:00, 501.75batch/s, mse=0.000768]\n",
      "Epoch 255: 100%|██████████| 46/46 [00:00<00:00, 384.80batch/s, mse=0.000767]\n",
      "Epoch 256: 100%|██████████| 46/46 [00:00<00:00, 496.45batch/s, mse=0.000767]\n",
      "Epoch 257: 100%|██████████| 46/46 [00:00<00:00, 396.54batch/s, mse=0.000767]\n",
      "Epoch 258: 100%|██████████| 46/46 [00:00<00:00, 525.35batch/s, mse=0.000766]\n",
      "Epoch 259: 100%|██████████| 46/46 [00:00<00:00, 309.50batch/s, mse=0.000766]\n",
      "Epoch 260: 100%|██████████| 46/46 [00:00<00:00, 488.40batch/s, mse=0.000766]\n",
      "Epoch 261: 100%|██████████| 46/46 [00:00<00:00, 408.61batch/s, mse=0.000765]\n",
      "Epoch 262: 100%|██████████| 46/46 [00:00<00:00, 435.75batch/s, mse=0.000765]\n",
      "Epoch 263: 100%|██████████| 46/46 [00:00<00:00, 416.02batch/s, mse=0.000765]\n",
      "Epoch 264: 100%|██████████| 46/46 [00:00<00:00, 444.19batch/s, mse=0.000764]\n",
      "Epoch 265: 100%|██████████| 46/46 [00:00<00:00, 444.28batch/s, mse=0.000764]\n",
      "Epoch 266: 100%|██████████| 46/46 [00:00<00:00, 425.04batch/s, mse=0.000764]\n",
      "Epoch 267: 100%|██████████| 46/46 [00:00<00:00, 456.90batch/s, mse=0.000764]\n",
      "Epoch 268: 100%|██████████| 46/46 [00:00<00:00, 415.05batch/s, mse=0.000763]\n",
      "Epoch 269: 100%|██████████| 46/46 [00:00<00:00, 481.47batch/s, mse=0.000763]\n",
      "Epoch 270: 100%|██████████| 46/46 [00:00<00:00, 387.68batch/s, mse=0.000763]\n",
      "Epoch 271: 100%|██████████| 46/46 [00:00<00:00, 483.97batch/s, mse=0.000762]\n",
      "Epoch 272: 100%|██████████| 46/46 [00:00<00:00, 361.65batch/s, mse=0.000762]\n",
      "Epoch 273: 100%|██████████| 46/46 [00:00<00:00, 418.53batch/s, mse=0.000762]\n",
      "Epoch 274: 100%|██████████| 46/46 [00:00<00:00, 331.87batch/s, mse=0.000761]\n",
      "Epoch 275: 100%|██████████| 46/46 [00:00<00:00, 352.28batch/s, mse=0.000761]\n",
      "Epoch 276: 100%|██████████| 46/46 [00:00<00:00, 351.47batch/s, mse=0.000761]\n",
      "Epoch 277: 100%|██████████| 46/46 [00:00<00:00, 381.49batch/s, mse=0.000761]\n",
      "Epoch 278: 100%|██████████| 46/46 [00:00<00:00, 380.58batch/s, mse=0.00076] \n",
      "Epoch 279: 100%|██████████| 46/46 [00:00<00:00, 370.84batch/s, mse=0.00076] \n",
      "Epoch 280: 100%|██████████| 46/46 [00:00<00:00, 403.72batch/s, mse=0.00076] \n",
      "Epoch 281: 100%|██████████| 46/46 [00:00<00:00, 338.34batch/s, mse=0.000759]\n",
      "Epoch 282: 100%|██████████| 46/46 [00:00<00:00, 419.75batch/s, mse=0.000759]\n",
      "Epoch 283: 100%|██████████| 46/46 [00:00<00:00, 343.52batch/s, mse=0.000759]\n",
      "Epoch 284: 100%|██████████| 46/46 [00:00<00:00, 412.04batch/s, mse=0.000759]\n",
      "Epoch 285: 100%|██████████| 46/46 [00:00<00:00, 350.22batch/s, mse=0.000758]\n",
      "Epoch 286: 100%|██████████| 46/46 [00:00<00:00, 439.62batch/s, mse=0.000758]\n",
      "Epoch 287: 100%|██████████| 46/46 [00:00<00:00, 345.22batch/s, mse=0.000758]\n",
      "Epoch 288: 100%|██████████| 46/46 [00:00<00:00, 404.31batch/s, mse=0.000757]\n",
      "Epoch 289: 100%|██████████| 46/46 [00:00<00:00, 341.31batch/s, mse=0.000757]\n",
      "Epoch 290: 100%|██████████| 46/46 [00:00<00:00, 391.72batch/s, mse=0.000757]\n",
      "Epoch 291: 100%|██████████| 46/46 [00:00<00:00, 359.69batch/s, mse=0.000756]\n",
      "Epoch 292: 100%|██████████| 46/46 [00:00<00:00, 453.95batch/s, mse=0.000756]\n",
      "Epoch 293: 100%|██████████| 46/46 [00:00<00:00, 447.00batch/s, mse=0.000756]\n",
      "Epoch 294: 100%|██████████| 46/46 [00:00<00:00, 430.64batch/s, mse=0.000756]\n",
      "Epoch 295: 100%|██████████| 46/46 [00:00<00:00, 446.68batch/s, mse=0.000755]\n",
      "Epoch 296: 100%|██████████| 46/46 [00:00<00:00, 421.56batch/s, mse=0.000755]\n",
      "Epoch 297: 100%|██████████| 46/46 [00:00<00:00, 487.30batch/s, mse=0.000755]\n",
      "Epoch 298: 100%|██████████| 46/46 [00:00<00:00, 387.83batch/s, mse=0.000755]\n",
      "Epoch 299: 100%|██████████| 46/46 [00:00<00:00, 455.69batch/s, mse=0.000754]\n",
      "Epoch 300: 100%|██████████| 46/46 [00:00<00:00, 406.45batch/s, mse=0.000754]\n",
      "Epoch 301: 100%|██████████| 46/46 [00:00<00:00, 496.89batch/s, mse=0.000754]\n",
      "Epoch 302: 100%|██████████| 46/46 [00:00<00:00, 394.92batch/s, mse=0.000753]\n",
      "Epoch 303: 100%|██████████| 46/46 [00:00<00:00, 377.64batch/s, mse=0.000753]\n",
      "Epoch 304: 100%|██████████| 46/46 [00:00<00:00, 352.37batch/s, mse=0.000753]\n",
      "Epoch 305: 100%|██████████| 46/46 [00:00<00:00, 465.61batch/s, mse=0.000753]\n",
      "Epoch 306: 100%|██████████| 46/46 [00:00<00:00, 374.31batch/s, mse=0.000752]\n",
      "Epoch 307: 100%|██████████| 46/46 [00:00<00:00, 513.30batch/s, mse=0.000752]\n",
      "Epoch 308: 100%|██████████| 46/46 [00:00<00:00, 387.55batch/s, mse=0.000752]\n",
      "Epoch 309: 100%|██████████| 46/46 [00:00<00:00, 486.81batch/s, mse=0.000752]\n",
      "Epoch 310: 100%|██████████| 46/46 [00:00<00:00, 367.48batch/s, mse=0.000751]\n",
      "Epoch 311: 100%|██████████| 46/46 [00:00<00:00, 462.25batch/s, mse=0.000751]\n",
      "Epoch 312: 100%|██████████| 46/46 [00:00<00:00, 418.52batch/s, mse=0.000751]\n",
      "Epoch 313: 100%|██████████| 46/46 [00:00<00:00, 469.72batch/s, mse=0.00075] \n",
      "Epoch 314: 100%|██████████| 46/46 [00:00<00:00, 424.79batch/s, mse=0.00075] \n",
      "Epoch 315: 100%|██████████| 46/46 [00:00<00:00, 428.32batch/s, mse=0.00075] \n",
      "Epoch 316: 100%|██████████| 46/46 [00:00<00:00, 361.53batch/s, mse=0.00075] \n",
      "Epoch 317: 100%|██████████| 46/46 [00:00<00:00, 378.37batch/s, mse=0.000749]\n",
      "Epoch 318: 100%|██████████| 46/46 [00:00<00:00, 493.99batch/s, mse=0.000749]\n",
      "Epoch 319: 100%|██████████| 46/46 [00:00<00:00, 384.79batch/s, mse=0.000749]\n",
      "Epoch 320: 100%|██████████| 46/46 [00:00<00:00, 486.49batch/s, mse=0.000749]\n",
      "Epoch 321: 100%|██████████| 46/46 [00:00<00:00, 384.95batch/s, mse=0.000748]\n",
      "Epoch 322: 100%|██████████| 46/46 [00:00<00:00, 501.73batch/s, mse=0.000748]\n",
      "Epoch 323: 100%|██████████| 46/46 [00:00<00:00, 373.84batch/s, mse=0.000748]\n",
      "Epoch 324: 100%|██████████| 46/46 [00:00<00:00, 513.30batch/s, mse=0.000748]\n",
      "Epoch 325: 100%|██████████| 46/46 [00:00<00:00, 380.23batch/s, mse=0.000747]\n",
      "Epoch 326: 100%|██████████| 46/46 [00:00<00:00, 531.18batch/s, mse=0.000747]\n",
      "Epoch 327: 100%|██████████| 46/46 [00:00<00:00, 401.63batch/s, mse=0.000747]\n",
      "Epoch 328: 100%|██████████| 46/46 [00:00<00:00, 443.68batch/s, mse=0.000747]\n",
      "Epoch 329: 100%|██████████| 46/46 [00:00<00:00, 445.94batch/s, mse=0.000746]\n",
      "Epoch 330: 100%|██████████| 46/46 [00:00<00:00, 312.62batch/s, mse=0.000746]\n",
      "Epoch 331: 100%|██████████| 46/46 [00:00<00:00, 499.91batch/s, mse=0.000746]\n",
      "Epoch 332: 100%|██████████| 46/46 [00:00<00:00, 386.00batch/s, mse=0.000746]\n",
      "Epoch 333: 100%|██████████| 46/46 [00:00<00:00, 590.24batch/s, mse=0.000745]\n",
      "Epoch 334: 100%|██████████| 46/46 [00:00<00:00, 379.39batch/s, mse=0.000745]\n",
      "Epoch 335: 100%|██████████| 46/46 [00:00<00:00, 404.40batch/s, mse=0.000745]\n",
      "Epoch 336: 100%|██████████| 46/46 [00:00<00:00, 473.97batch/s, mse=0.000745]\n",
      "Epoch 337: 100%|██████████| 46/46 [00:00<00:00, 344.18batch/s, mse=0.000744]\n",
      "Epoch 338: 100%|██████████| 46/46 [00:00<00:00, 531.48batch/s, mse=0.000744]\n",
      "Epoch 339: 100%|██████████| 46/46 [00:00<00:00, 382.79batch/s, mse=0.000744]\n",
      "Epoch 340: 100%|██████████| 46/46 [00:00<00:00, 518.70batch/s, mse=0.000744]\n",
      "Epoch 341: 100%|██████████| 46/46 [00:00<00:00, 375.18batch/s, mse=0.000743]\n",
      "Epoch 342: 100%|██████████| 46/46 [00:00<00:00, 486.59batch/s, mse=0.000743]\n",
      "Epoch 343: 100%|██████████| 46/46 [00:00<00:00, 312.59batch/s, mse=0.000743]\n",
      "Epoch 344: 100%|██████████| 46/46 [00:00<00:00, 480.98batch/s, mse=0.000743]\n",
      "Epoch 345: 100%|██████████| 46/46 [00:00<00:00, 392.33batch/s, mse=0.000742]\n",
      "Epoch 346: 100%|██████████| 46/46 [00:00<00:00, 397.02batch/s, mse=0.000742]\n",
      "Epoch 347: 100%|██████████| 46/46 [00:00<00:00, 421.84batch/s, mse=0.000742]\n",
      "Epoch 348: 100%|██████████| 46/46 [00:00<00:00, 350.82batch/s, mse=0.000742]\n",
      "Epoch 349: 100%|██████████| 46/46 [00:00<00:00, 453.75batch/s, mse=0.000741]\n",
      "Epoch 350: 100%|██████████| 46/46 [00:00<00:00, 379.47batch/s, mse=0.000741]\n",
      "Epoch 351: 100%|██████████| 46/46 [00:00<00:00, 459.65batch/s, mse=0.000741]\n",
      "Epoch 352: 100%|██████████| 46/46 [00:00<00:00, 382.25batch/s, mse=0.000741]\n",
      "Epoch 353: 100%|██████████| 46/46 [00:00<00:00, 397.14batch/s, mse=0.00074] \n",
      "Epoch 354: 100%|██████████| 46/46 [00:00<00:00, 411.54batch/s, mse=0.00074] \n",
      "Epoch 355: 100%|██████████| 46/46 [00:00<00:00, 317.48batch/s, mse=0.00074] \n",
      "Epoch 356: 100%|██████████| 46/46 [00:00<00:00, 418.69batch/s, mse=0.00074] \n",
      "Epoch 357: 100%|██████████| 46/46 [00:00<00:00, 354.53batch/s, mse=0.00074] \n",
      "Epoch 358: 100%|██████████| 46/46 [00:00<00:00, 525.51batch/s, mse=0.000739]\n",
      "Epoch 359: 100%|██████████| 46/46 [00:00<00:00, 375.59batch/s, mse=0.000739]\n",
      "Epoch 360: 100%|██████████| 46/46 [00:00<00:00, 471.35batch/s, mse=0.000739]\n",
      "Epoch 361: 100%|██████████| 46/46 [00:00<00:00, 420.06batch/s, mse=0.000739]\n",
      "Epoch 362: 100%|██████████| 46/46 [00:00<00:00, 409.27batch/s, mse=0.000738]\n",
      "Epoch 363: 100%|██████████| 46/46 [00:00<00:00, 475.91batch/s, mse=0.000738]\n",
      "Epoch 364: 100%|██████████| 46/46 [00:00<00:00, 385.15batch/s, mse=0.000738]\n",
      "Epoch 365: 100%|██████████| 46/46 [00:00<00:00, 501.80batch/s, mse=0.000738]\n",
      "Epoch 366: 100%|██████████| 46/46 [00:00<00:00, 382.87batch/s, mse=0.000737]\n",
      "Epoch 367: 100%|██████████| 46/46 [00:00<00:00, 485.81batch/s, mse=0.000737]\n",
      "Epoch 368: 100%|██████████| 46/46 [00:00<00:00, 353.34batch/s, mse=0.000737]\n",
      "Epoch 369: 100%|██████████| 46/46 [00:00<00:00, 360.45batch/s, mse=0.000737]\n",
      "Epoch 370: 100%|██████████| 46/46 [00:00<00:00, 452.86batch/s, mse=0.000737]\n",
      "Epoch 371: 100%|██████████| 46/46 [00:00<00:00, 426.42batch/s, mse=0.000736]\n",
      "Epoch 372: 100%|██████████| 46/46 [00:00<00:00, 485.55batch/s, mse=0.000736]\n",
      "Epoch 373: 100%|██████████| 46/46 [00:00<00:00, 356.46batch/s, mse=0.000736]\n",
      "Epoch 374: 100%|██████████| 46/46 [00:00<00:00, 394.53batch/s, mse=0.000736]\n",
      "Epoch 375: 100%|██████████| 46/46 [00:00<00:00, 300.44batch/s, mse=0.000735]\n",
      "Epoch 376: 100%|██████████| 46/46 [00:00<00:00, 388.96batch/s, mse=0.000735]\n",
      "Epoch 377: 100%|██████████| 46/46 [00:00<00:00, 390.60batch/s, mse=0.000735]\n",
      "Epoch 378: 100%|██████████| 46/46 [00:00<00:00, 359.53batch/s, mse=0.000735]\n",
      "Epoch 379: 100%|██████████| 46/46 [00:00<00:00, 468.35batch/s, mse=0.000734]\n",
      "Epoch 380: 100%|██████████| 46/46 [00:00<00:00, 314.64batch/s, mse=0.000734]\n",
      "Epoch 381: 100%|██████████| 46/46 [00:00<00:00, 309.91batch/s, mse=0.000734]\n",
      "Epoch 382: 100%|██████████| 46/46 [00:00<00:00, 306.48batch/s, mse=0.000734]\n",
      "Epoch 383: 100%|██████████| 46/46 [00:00<00:00, 395.49batch/s, mse=0.000734]\n",
      "Epoch 384: 100%|██████████| 46/46 [00:00<00:00, 357.70batch/s, mse=0.000733]\n",
      "Epoch 385: 100%|██████████| 46/46 [00:00<00:00, 387.63batch/s, mse=0.000733]\n",
      "Epoch 386: 100%|██████████| 46/46 [00:00<00:00, 437.07batch/s, mse=0.000733]\n",
      "Epoch 387: 100%|██████████| 46/46 [00:00<00:00, 349.08batch/s, mse=0.000733]\n",
      "Epoch 388: 100%|██████████| 46/46 [00:00<00:00, 443.98batch/s, mse=0.000733]\n",
      "Epoch 389: 100%|██████████| 46/46 [00:00<00:00, 361.68batch/s, mse=0.000732]\n",
      "Epoch 390: 100%|██████████| 46/46 [00:00<00:00, 443.97batch/s, mse=0.000732]\n",
      "Epoch 391: 100%|██████████| 46/46 [00:00<00:00, 358.58batch/s, mse=0.000732]\n",
      "Epoch 392: 100%|██████████| 46/46 [00:00<00:00, 352.89batch/s, mse=0.000732]\n",
      "Epoch 393: 100%|██████████| 46/46 [00:00<00:00, 412.31batch/s, mse=0.000731]\n",
      "Epoch 394: 100%|██████████| 46/46 [00:00<00:00, 393.67batch/s, mse=0.000731]\n",
      "Epoch 395: 100%|██████████| 46/46 [00:00<00:00, 444.67batch/s, mse=0.000731]\n",
      "Epoch 396: 100%|██████████| 46/46 [00:00<00:00, 273.60batch/s, mse=0.000731]\n",
      "Epoch 397: 100%|██████████| 46/46 [00:00<00:00, 365.86batch/s, mse=0.000731]\n",
      "Epoch 398: 100%|██████████| 46/46 [00:00<00:00, 286.39batch/s, mse=0.00073] \n",
      "Epoch 399: 100%|██████████| 46/46 [00:00<00:00, 273.83batch/s, mse=0.00073] \n",
      "Epoch 400: 100%|██████████| 46/46 [00:00<00:00, 307.30batch/s, mse=0.00073] \n",
      "Epoch 401: 100%|██████████| 46/46 [00:00<00:00, 286.62batch/s, mse=0.00073] \n",
      "Epoch 402: 100%|██████████| 46/46 [00:00<00:00, 372.31batch/s, mse=0.00073] \n",
      "Epoch 403: 100%|██████████| 46/46 [00:00<00:00, 371.78batch/s, mse=0.000729]\n",
      "Epoch 404: 100%|██████████| 46/46 [00:00<00:00, 324.56batch/s, mse=0.000729]\n",
      "Epoch 405: 100%|██████████| 46/46 [00:00<00:00, 373.51batch/s, mse=0.000729]\n",
      "Epoch 406: 100%|██████████| 46/46 [00:00<00:00, 335.80batch/s, mse=0.000729]\n",
      "Epoch 407: 100%|██████████| 46/46 [00:00<00:00, 437.56batch/s, mse=0.000729]\n",
      "Epoch 408: 100%|██████████| 46/46 [00:00<00:00, 341.62batch/s, mse=0.000728]\n",
      "Epoch 409: 100%|██████████| 46/46 [00:00<00:00, 394.38batch/s, mse=0.000728]\n",
      "Epoch 410: 100%|██████████| 46/46 [00:00<00:00, 333.67batch/s, mse=0.000728]\n",
      "Epoch 411: 100%|██████████| 46/46 [00:00<00:00, 389.65batch/s, mse=0.000728]\n",
      "Epoch 412: 100%|██████████| 46/46 [00:00<00:00, 346.56batch/s, mse=0.000727]\n",
      "Epoch 413: 100%|██████████| 46/46 [00:00<00:00, 380.89batch/s, mse=0.000727]\n",
      "Epoch 414: 100%|██████████| 46/46 [00:00<00:00, 362.93batch/s, mse=0.000727]\n",
      "Epoch 415: 100%|██████████| 46/46 [00:00<00:00, 380.91batch/s, mse=0.000727]\n",
      "Epoch 416: 100%|██████████| 46/46 [00:00<00:00, 322.40batch/s, mse=0.000727]\n",
      "Epoch 417: 100%|██████████| 46/46 [00:00<00:00, 382.72batch/s, mse=0.000726]\n",
      "Epoch 418: 100%|██████████| 46/46 [00:00<00:00, 400.30batch/s, mse=0.000726]\n",
      "Epoch 419: 100%|██████████| 46/46 [00:00<00:00, 375.27batch/s, mse=0.000726]\n",
      "Epoch 420: 100%|██████████| 46/46 [00:00<00:00, 502.57batch/s, mse=0.000726]\n",
      "Epoch 421: 100%|██████████| 46/46 [00:00<00:00, 361.57batch/s, mse=0.000726]\n",
      "Epoch 422: 100%|██████████| 46/46 [00:00<00:00, 475.50batch/s, mse=0.000725]\n",
      "Epoch 423: 100%|██████████| 46/46 [00:00<00:00, 476.53batch/s, mse=0.000725]\n",
      "Epoch 424: 100%|██████████| 46/46 [00:00<00:00, 391.89batch/s, mse=0.000725]\n",
      "Epoch 425: 100%|██████████| 46/46 [00:00<00:00, 416.34batch/s, mse=0.000725]\n",
      "Epoch 426: 100%|██████████| 46/46 [00:00<00:00, 450.79batch/s, mse=0.000725]\n",
      "Epoch 427: 100%|██████████| 46/46 [00:00<00:00, 383.16batch/s, mse=0.000724]\n",
      "Epoch 428: 100%|██████████| 46/46 [00:00<00:00, 534.81batch/s, mse=0.000724]\n",
      "Epoch 429: 100%|██████████| 46/46 [00:00<00:00, 322.41batch/s, mse=0.000724]\n",
      "Epoch 430: 100%|██████████| 46/46 [00:00<00:00, 424.22batch/s, mse=0.000724]\n",
      "Epoch 431: 100%|██████████| 46/46 [00:00<00:00, 427.71batch/s, mse=0.000724]\n",
      "Epoch 432: 100%|██████████| 46/46 [00:00<00:00, 394.72batch/s, mse=0.000724]\n",
      "Epoch 433: 100%|██████████| 46/46 [00:00<00:00, 433.03batch/s, mse=0.000723]\n",
      "Epoch 434: 100%|██████████| 46/46 [00:00<00:00, 462.11batch/s, mse=0.000723]\n",
      "Epoch 435: 100%|██████████| 46/46 [00:00<00:00, 427.51batch/s, mse=0.000723]\n",
      "Epoch 436: 100%|██████████| 46/46 [00:00<00:00, 364.44batch/s, mse=0.000723]\n",
      "Epoch 437: 100%|██████████| 46/46 [00:00<00:00, 496.26batch/s, mse=0.000723]\n",
      "Epoch 438: 100%|██████████| 46/46 [00:00<00:00, 372.87batch/s, mse=0.000722]\n",
      "Epoch 439: 100%|██████████| 46/46 [00:00<00:00, 497.13batch/s, mse=0.000722]\n",
      "Epoch 440: 100%|██████████| 46/46 [00:00<00:00, 374.83batch/s, mse=0.000722]\n",
      "Epoch 441: 100%|██████████| 46/46 [00:00<00:00, 497.06batch/s, mse=0.000722]\n",
      "Epoch 442: 100%|██████████| 46/46 [00:00<00:00, 359.61batch/s, mse=0.000722]\n",
      "Epoch 443: 100%|██████████| 46/46 [00:00<00:00, 386.04batch/s, mse=0.000721]\n",
      "Epoch 444: 100%|██████████| 46/46 [00:00<00:00, 396.08batch/s, mse=0.000721]\n",
      "Epoch 445: 100%|██████████| 46/46 [00:00<00:00, 348.34batch/s, mse=0.000721]\n",
      "Epoch 446: 100%|██████████| 46/46 [00:00<00:00, 437.50batch/s, mse=0.000721]\n",
      "Epoch 447: 100%|██████████| 46/46 [00:00<00:00, 374.29batch/s, mse=0.000721]\n",
      "Epoch 448: 100%|██████████| 46/46 [00:00<00:00, 516.99batch/s, mse=0.00072] \n",
      "Epoch 449: 100%|██████████| 46/46 [00:00<00:00, 402.53batch/s, mse=0.00072] \n",
      "Epoch 450: 100%|██████████| 46/46 [00:00<00:00, 496.93batch/s, mse=0.00072] \n",
      "Epoch 451: 100%|██████████| 46/46 [00:00<00:00, 415.53batch/s, mse=0.00072] \n",
      "Epoch 452: 100%|██████████| 46/46 [00:00<00:00, 419.26batch/s, mse=0.00072] \n",
      "Epoch 453: 100%|██████████| 46/46 [00:00<00:00, 476.52batch/s, mse=0.00072] \n",
      "Epoch 454: 100%|██████████| 46/46 [00:00<00:00, 385.22batch/s, mse=0.000719]\n",
      "Epoch 455: 100%|██████████| 46/46 [00:00<00:00, 522.52batch/s, mse=0.000719]\n",
      "Epoch 456: 100%|██████████| 46/46 [00:00<00:00, 388.09batch/s, mse=0.000719]\n",
      "Epoch 457: 100%|██████████| 46/46 [00:00<00:00, 414.69batch/s, mse=0.000719]\n",
      "Epoch 458: 100%|██████████| 46/46 [00:00<00:00, 475.98batch/s, mse=0.000719]\n",
      "Epoch 459: 100%|██████████| 46/46 [00:00<00:00, 405.01batch/s, mse=0.000718]\n",
      "Epoch 460: 100%|██████████| 46/46 [00:00<00:00, 476.04batch/s, mse=0.000718]\n",
      "Epoch 461: 100%|██████████| 46/46 [00:00<00:00, 404.14batch/s, mse=0.000718]\n",
      "Epoch 462: 100%|██████████| 46/46 [00:00<00:00, 486.12batch/s, mse=0.000718]\n",
      "Epoch 463: 100%|██████████| 46/46 [00:00<00:00, 380.73batch/s, mse=0.000718]\n",
      "Epoch 464: 100%|██████████| 46/46 [00:00<00:00, 466.79batch/s, mse=0.000718]\n",
      "Epoch 465: 100%|██████████| 46/46 [00:00<00:00, 377.62batch/s, mse=0.000717]\n",
      "Epoch 466: 100%|██████████| 46/46 [00:00<00:00, 458.66batch/s, mse=0.000717]\n",
      "Epoch 467: 100%|██████████| 46/46 [00:00<00:00, 342.16batch/s, mse=0.000717]\n",
      "Epoch 468: 100%|██████████| 46/46 [00:00<00:00, 380.83batch/s, mse=0.000717]\n",
      "Epoch 469: 100%|██████████| 46/46 [00:00<00:00, 528.70batch/s, mse=0.000717]\n",
      "Epoch 470: 100%|██████████| 46/46 [00:00<00:00, 390.03batch/s, mse=0.000716]\n",
      "Epoch 471: 100%|██████████| 46/46 [00:00<00:00, 403.58batch/s, mse=0.000716]\n",
      "Epoch 472: 100%|██████████| 46/46 [00:00<00:00, 442.13batch/s, mse=0.000716]\n",
      "Epoch 473: 100%|██████████| 46/46 [00:00<00:00, 378.59batch/s, mse=0.000716]\n",
      "Epoch 474: 100%|██████████| 46/46 [00:00<00:00, 542.35batch/s, mse=0.000716]\n",
      "Epoch 475: 100%|██████████| 46/46 [00:00<00:00, 367.02batch/s, mse=0.000716]\n",
      "Epoch 476: 100%|██████████| 46/46 [00:00<00:00, 511.76batch/s, mse=0.000715]\n",
      "Epoch 477: 100%|██████████| 46/46 [00:00<00:00, 375.81batch/s, mse=0.000715]\n",
      "Epoch 478: 100%|██████████| 46/46 [00:00<00:00, 374.11batch/s, mse=0.000715]\n",
      "Epoch 479: 100%|██████████| 46/46 [00:00<00:00, 517.03batch/s, mse=0.000715]\n",
      "Epoch 480: 100%|██████████| 46/46 [00:00<00:00, 338.69batch/s, mse=0.000715]\n",
      "Epoch 481: 100%|██████████| 46/46 [00:00<00:00, 463.42batch/s, mse=0.000715]\n",
      "Epoch 482: 100%|██████████| 46/46 [00:00<00:00, 341.36batch/s, mse=0.000714]\n",
      "Epoch 483: 100%|██████████| 46/46 [00:00<00:00, 455.40batch/s, mse=0.000714]\n",
      "Epoch 484: 100%|██████████| 46/46 [00:00<00:00, 343.97batch/s, mse=0.000714]\n",
      "Epoch 485: 100%|██████████| 46/46 [00:00<00:00, 438.34batch/s, mse=0.000714]\n",
      "Epoch 486: 100%|██████████| 46/46 [00:00<00:00, 367.74batch/s, mse=0.000714]\n",
      "Epoch 487: 100%|██████████| 46/46 [00:00<00:00, 401.22batch/s, mse=0.000714]\n",
      "Epoch 488: 100%|██████████| 46/46 [00:00<00:00, 390.17batch/s, mse=0.000713]\n",
      "Epoch 489: 100%|██████████| 46/46 [00:00<00:00, 342.05batch/s, mse=0.000713]\n",
      "Epoch 490: 100%|██████████| 46/46 [00:00<00:00, 416.12batch/s, mse=0.000713]\n",
      "Epoch 491: 100%|██████████| 46/46 [00:00<00:00, 361.50batch/s, mse=0.000713]\n",
      "Epoch 492: 100%|██████████| 46/46 [00:00<00:00, 441.29batch/s, mse=0.000713]\n",
      "Epoch 493: 100%|██████████| 46/46 [00:00<00:00, 372.52batch/s, mse=0.000713]\n",
      "Epoch 494: 100%|██████████| 46/46 [00:00<00:00, 512.46batch/s, mse=0.000712]\n",
      "Epoch 495: 100%|██████████| 46/46 [00:00<00:00, 423.86batch/s, mse=0.000712]\n",
      "Epoch 496: 100%|██████████| 46/46 [00:00<00:00, 472.61batch/s, mse=0.000712]\n",
      "Epoch 497: 100%|██████████| 46/46 [00:00<00:00, 402.25batch/s, mse=0.000712]\n",
      "Epoch 498: 100%|██████████| 46/46 [00:00<00:00, 479.78batch/s, mse=0.000712]\n",
      "Epoch 499: 100%|██████████| 46/46 [00:00<00:00, 373.56batch/s, mse=0.000712]\n"
     ]
    }
   ],
   "source": [
    "nn_train(X_train = X_train, y_train= y_train, model=model, loss_fn = loss_fn, optimizer_name = optimizer_name, lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, batch_size=batch_size, log=False, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033063103910535574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open('models/NN/59_NN.pkl', 'rb'))\n",
    "y_test_pred = model(X_test)\n",
    "test_loss = loss_fn(y_test_pred, y_test)\n",
    "test_loss = float(test_loss)\n",
    "test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
